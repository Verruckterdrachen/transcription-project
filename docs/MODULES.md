# üìö –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –º–æ–¥—É–ª–µ–π –∏ —Ñ—É–Ω–∫—Ü–∏–π
# –û–±–Ω–æ–≤–ª–µ–Ω–æ: 2026-02-20

–ü–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≤—Å–µ—Ö —Ñ—É–Ω–∫—Ü–∏–π –ø—Ä–æ–µ–∫—Ç–∞ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.

---

## üìÇ scripts/core/ - –ë–∞–∑–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å

### config.py

**–û–ø–∏—Å–∞–Ω–∏–µ:** –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã

```python
HF_TOKEN: str
    Hugging Face API —Ç–æ–∫–µ–Ω –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –º–æ–¥–µ–ª—è–º

VERSION: str
    –¢–µ–∫—É—â–∞—è –≤–µ—Ä—Å–∏—è –ø—Ä–æ–µ–∫—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: "17.9")

VERSION_NAME: str
    –ö–æ–¥–æ–≤–æ–µ –∏–º—è –≤–µ—Ä—Å–∏–∏
utils.py
seconds_to_hms(seconds: float) -> str
–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å–µ–∫—É–Ω–¥—ã –≤ —Ñ–æ—Ä–º–∞—Ç HH:MM:SS.mmm

python
seconds_to_hms(125.456)
# ‚Üí "00:02:05.456"
parse_speaker_folder(folder_name: str) -> Tuple[str, str, str]
–ü–∞—Ä—Å–∏—Ç –∏–º—è –ø–∞–ø–∫–∏ —Å–ø–∏–∫–µ—Ä–∞

–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:

folder_name: –ò–º—è –ø–∞–ø–∫–∏ (—Ñ–æ—Ä–º–∞—Ç: "–§–∞–º–∏–ª–∏—è (–î–î.–ú–ú)")

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (surname, date, full_name)

python
parse_speaker_folder("–ò—Å–∞–µ–≤ (29.01)")
# ‚Üí ("–ò—Å–∞–µ–≤", "29.01", "–ò—Å–∞–µ–≤")
adaptive_vad_threshold(duration: float) -> float
–í—ã—á–∏—Å–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π VAD threshold –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

python
if duration < 600:      # <10 –º–∏–Ω ‚Üí 0.3
elif duration < 1800:   # 10-30 –º–∏–Ω ‚Üí 0.4
else:                   # >30 –º–∏–Ω ‚Üí 0.5
gap_detector(segments: List[Dict], threshold: float = 3.0) -> List[Dict]
–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –ø–∞—É–∑—ã (gaps) –º–µ–∂–¥—É —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:

python
{
    "gap_hms_start": "00:05:30.000",
    "gap_hms_end": "00:05:35.000",
    "start": 330.0,
    "end": 335.0,
    "duration": 5.0
}
text_similarity(text1: str, text2: str) -> float
üÜï v16.5: –í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤

–ê–ª–≥–æ—Ä–∏—Ç–º: Jaccard similarity –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤

python
text_similarity("–î–æ–±—Ä—ã–π –¥–µ–Ω—å", "–î–æ–±—Ä—ã–π –≤–µ—á–µ—Ä")
# ‚Üí 0.67 (2 –æ–±—â–∏—Ö —Å–ª–æ–≤–∞ –∏–∑ 3)
diarization.py
diarize_audio(pipeline, audio_path: Path, min_speakers: int, max_speakers: int) -> Annotation
–í—ã–ø–æ–ª–Ω—è–µ—Ç –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞

python
pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization-3.1")
diarization = diarize_audio(pipeline, Path("audio.wav"), 2, 3)
compute_speaker_stats(diarization: Annotation) -> Dict[str, Dict]
–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞–∂–¥–æ–º—É —Å–ø–∏–∫–µ—Ä—É

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:

python
{
    "SPEAKER_00": {
        "total_time": 1234.5,
        "num_segments": 89
    }
}
identify_speaker_roles(stats: Dict, segments: List[Dict], speaker_surname: str = "") -> Dict[str, str]
üîß v17.8: –ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ä–æ–ª–∏ —Å–ø–∏–∫–µ—Ä–æ–≤ (–ñ—É—Ä–Ω–∞–ª–∏—Å—Ç / –≠–∫—Å–ø–µ—Ä—Ç / –û–ø–µ—Ä–∞—Ç–æ—Ä)

–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:

stats: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–ø–∏–∫–µ—Ä–æ–≤

segments: –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤

speaker_surname: –§–∞–º–∏–ª–∏—è –≥–ª–∞–≤–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞ (–¥–æ–±–∞–≤–ª–µ–Ω–æ v17.8, –ë–ê–ì #26)

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:

python
{
    "SPEAKER_00": "–ñ—É—Ä–Ω–∞–ª–∏—Å—Ç",
    "SPEAKER_01": "–ì–æ—Ä–±—É–Ω–æ–≤",   # ‚Üê speaker_surname, –Ω–µ "–°–ø–∏–∫–µ—Ä"
    "SPEAKER_02": "–û–ø–µ—Ä–∞—Ç–æ—Ä"    # –µ—Å–ª–∏ –µ—Å—Ç—å
}
–õ–æ–≥–∏–∫–∞:

–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è ‚Üí –ñ—É—Ä–Ω–∞–ª–∏—Å—Ç

–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è ‚Üí speaker_surname (fallback: "–°–ø–∏–∫–µ—Ä" –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω)

–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π ‚Üí –û–ø–µ—Ä–∞—Ç–æ—Ä

‚ö†Ô∏è –ë–ê–ì #26 (v17.8): –¥–æ —Ñ–∏–∫—Å–∞ hardcoded "–°–ø–∏–∫–µ—Ä" –≤–º–µ—Å—Ç–æ speaker_surname.

align_segment_to_diarization(segment: Dict, diarization: Annotation) -> str
–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–ø–∏–∫–µ—Ä–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: ID —Å–ø–∏–∫–µ—Ä–∞ ("SPEAKER_00", "SPEAKER_01", ...)

alignment.py
align_whisper_with_diarization(segments, diarization, speaker_surname, speaker_roles) -> List[Dict]
–°–≤—è–∑—ã–≤–∞–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é Whisper —Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:

python
{
    "start": 0.0,
    "end": 5.3,
    "text": "–î–æ–±—Ä—ã–π –¥–µ–Ω—å!",
    "speaker": "–ñ—É—Ä–Ω–∞–ª–∏—Å—Ç",
    "raw_speaker_id": "SPEAKER_00",
    "confidence": 0.95
}
transcription.py
transcribe_audio(model, audio_path, language, temperature, beam_size, vad_threshold) -> Dict
–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é Whisper

–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:

temperature: 0.0 –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏–∑–º–∞

beam_size: 5 –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞

force_transcribe_diar_gaps(model, audio_path, gaps, segments, speaker_surname) -> List[Dict]
üÜï v16.5: –ó–∞–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–æ–ø—É—Å–∫–∏ (gaps) —Å —É–º–Ω–æ–π –∞—Ç—Ä–∏–±—É—Ü–∏–µ–π

–ê–ª–≥–æ—Ä–∏—Ç–º:

python
gap_text = transcribe_gap_audio()

similarity_prev = text_similarity(gap_text, prev_segment["text"])
similarity_next = text_similarity(gap_text, next_segment["text"])

if similarity_prev > similarity_next:
    speaker = prev_segment["speaker"]
else:
    speaker = next_segment["speaker"]

# –ó–∞—â–∏—Ç–∞ –æ—Ç –∑–∞–ø–∏–Ω–æ–∫
if len(gap_text) < 10 or confidence < 0.6:
    speaker = diarization_speaker
üìÇ scripts/corrections/ - –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
hallucinations.py
filter_hallucination_segments(segments: List[Dict]) -> List[Dict]
–§–∏–ª—å—Ç—Ä—É–µ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ Whisper

–ö—Ä–∏—Ç–µ—Ä–∏–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏:

–ü–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è —Ç–µ–∫—Å—Ç (>3 —Ä–∞–∑–∞)

–ù–∏–∑–∫–∞—è confidence (<0.4)

–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã

clean_hallucinations_from_text(text: str) -> str
–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π

–ü—Ä–∏–º–µ—Ä—ã —É–¥–∞–ª—è–µ–º–æ–≥–æ:

"–°—É–±—Ç–∏—Ç—Ä—ã —Å–¥–µ–ª–∞–ª DimaTorzok"

"–ü–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å –Ω–∞ –∫–∞–Ω–∞–ª"

–ü–æ–≤—Ç–æ—Ä—ã —Ñ—Ä–∞–∑

speaker_classifier.py
apply_speaker_classification_v15(segments, speaker_surname, debug=False) -> Tuple[List[Dict], Dict]
–í–µ—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–ø–∏–∫–µ—Ä–æ–≤

–°–∏—Å—Ç–µ–º–∞ –≤–µ—Å–æ–≤:

python
{
    "duration":     0.30,
    "position":     0.15,
    "pattern":      0.25,
    "context":      0.20,
    "diarization":  0.10
}
boundary_fixer.py
boundary_correction_raw(segments, speaker_surname, speaker_roles) -> List[Dict]
–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã –º–µ–∂–¥—É —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏

–ß—Ç–æ –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç:

–ü–µ—Ä–µ–∫—Ä—ã—Ç–∏—è —Ç–∞–π–º–∫–æ–¥–æ–≤

–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ/–¥–ª–∏–Ω–Ω—ã–µ –ø–∞—É–∑—ã

–†–∞–∑—Ä—ã–≤—ã –≤ –¥–∏–∞–ª–æ–≥–µ

split_mixed_speaker_segments(segments, speaker_surname) -> List[Dict]
üîß v16.4: –†–∞–∑–¥–µ–ª—è–µ—Ç mixed-speaker —Å–µ–≥–º–µ–Ω—Ç—ã —Å –ø–µ—Ä–µ—Å—á—ë—Ç–æ–º —Ç–∞–π–º–∫–æ–¥–æ–≤

python
# –î–æ:
[{"text": "–î–∞, –ø–æ–Ω—è—Ç–Ω–æ. [–≤–º–µ—à–∏–≤–∞–µ—Ç—Å—è] –ü—Ä–æ–¥–æ–ª–∂—É", "speaker": "–≠–∫—Å–ø–µ—Ä—Ç"}]

# –ü–æ—Å–ª–µ:
[
    {"text": "–î–∞, –ø–æ–Ω—è—Ç–Ω–æ.",     "speaker": "–≠–∫—Å–ø–µ—Ä—Ç"},
    {"text": "–≤–º–µ—à–∏–≤–∞–µ—Ç—Å—è",      "speaker": "–ñ—É—Ä–Ω–∞–ª–∏—Å—Ç"},
    {"text": "–ü—Ä–æ–¥–æ–ª–∂—É",         "speaker": "–≠–∫—Å–ø–µ—Ä—Ç"}
]
journalist_commands.py
detect_journalist_commands_cross_segment(segments, speaker_surname) -> Tuple[List[Dict], List[Dict]]
–î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç –∫–æ–º–∞–Ω–¥—ã –ñ—É—Ä–Ω–∞–ª–∏—Å—Ç–∞, –ø—Ä–∏–ø–∏—Å–∞–Ω–Ω—ã–µ –≠–∫—Å–ø–µ—Ä—Ç—É

–î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:

python
["–¥–∞–≤–∞–π—Ç–µ", "–ø–æ–∂–∞–ª—É–π—Å—Ç–∞", "—Ä–∞—Å—Å–∫–∞–∂–∏—Ç–µ", "–º–æ–∂–µ—Ç–µ", "–æ–±—ä—è—Å–Ω–∏—Ç–µ", "–ø–æ–∫–∞–∂–∏—Ç–µ"]
text_corrections.py
text_based_correction(segments, speaker_surname) -> List[Dict]
üîß v16.4: –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Ç–µ–∫—Å—Ç–æ–≤–∞—è –∫–æ—Ä—Ä–µ–∫—Ü–∏—è

python
# Context Window Protection
if in_long_monologue(segment, threshold=60):
    skip_correction()

# Confirmation Detection
if text in ["–Ω—É –¥–∞", "–¥–∞-–¥–∞", "—É–≥—É"] and duration < 2:
    force_journalist()

# Announcement vs Question
if text.startswith("–¥–∞–≤–∞–π—Ç–µ") and len(text) < 50:
    force_journalist()
üìÇ scripts/merge/ - –°–ª–∏—è–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è
replica_merger.py
RUSSIAN_STOP_WORDS: set
üÜï v17.9: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –∏–∑ 104 –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ª–æ–≤ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ _count_meaningful() –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ N-–≥—Ä–∞–º–º –±–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏.

python
RUSSIAN_STOP_WORDS = {
    "–∏", "–≤", "–Ω–µ", "–Ω–∞", "—è", "—á—Ç–æ", "—Ç–æ—Ç", "–±—ã—Ç—å", "–æ–Ω",
    "—Å", "–∞", "–∫–∞–∫", "—ç—Ç–æ", "–Ω–æ", "–∫", "—É", "–∂–µ", "–∏–∑",
    "–∑–∞", "—Ç–æ", "–ø–æ", "–≤—Å–µ", "–±—ã–ª", "–±—ã–ª–∞", "–±—ã–ª–æ", "–±—ã–ª–∏",
    "–≤–æ—Ç", "–Ω–µ—Ç", "–¥–∞", "—É–∂–µ", "–µ—â—ë", "–µ—â–µ", "–≤—ã", "–º—ã",
    # ... 104 —Å–ª–æ–≤–∞
}
_count_meaningful(phrase: str) -> int
üÜï v17.9: –°—á–∏—Ç–∞–µ—Ç —Å–ª–æ–≤–∞ –≤ —Ñ—Ä–∞–∑–µ, –Ω–µ –≤—Ö–æ–¥—è—â–∏–µ –≤ RUSSIAN_STOP_WORDS

–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:

phrase: –°—Ç—Ä–æ–∫–∞ (N-–≥—Ä–∞–º–º–∞ –∏–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–∞—è —Ñ—Ä–∞–∑–∞)

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: int ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤

python
_count_meaningful("–±—ã–ª. –ò –≤–æ—Ç")   # ‚Üí 0  (–≤—Å–µ —Å–ª–æ–≤–∞ ‚Äî —Å—Ç–æ–ø-—Å–ª–æ–≤–∞)
_count_meaningful("–Ω–µ–º–µ—Ü–∫–∞—è –∞—Ä—Ç–∏–ª–ª–µ—Ä–∏—è –≤–ø—Ä–∞–≤—å")  # ‚Üí 3
‚ö†Ô∏è –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –±–µ—Å—Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã—Ö N-–≥—Ä–∞–º–º
–≤ seen[] –≤–Ω—É—Ç—Ä–∏ clean_loops() ‚Üí –∏—Å–∫–ª—é—á–∞–µ—Ç –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è.

clean_loops(text: str) -> str
üîß v17.9: –£–¥–∞–ª—è–µ—Ç loop-–∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã (–ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è N-–≥—Ä–∞–º–º—ã) –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Ä–µ–ø–ª–∏–∫–∏

–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:

text: –¢–µ–∫—Å—Ç –æ–¥–Ω–æ–π —Ä–µ–ø–ª–∏–∫–∏ –ø–æ—Å–ª–µ merge

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç

–ê–ª–≥–æ—Ä–∏—Ç–º (v17.9):

python
MIN_MEANINGFUL_WORDS = 2

for ngram in sliding_window(text, N=4):
    meaningful_count = _count_meaningful(ngram)

    if meaningful_count < MIN_MEANINGFUL_WORDS:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ output, –Ω–æ –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º –≤ seen[]
        # ‚Üí –Ω–µ –º–æ–∂–µ—Ç —Å—Ç–∞—Ç—å —è–∫–æ—Ä–µ–º –¥–ª—è –ª–æ–∂–Ω–æ–≥–æ fuzzy match
        output.append(ngram)
        continue

    if ngram in seen[] and fuzzy_sim(ngram, anchor) >= 0.75:
        # –†–µ–∞–ª—å–Ω—ã–π loop ‚Äî —É–¥–∞–ª—è–µ–º
        skip()
    else:
        seen[ngram] = ngram
        output.append(ngram)
–ì—Ä–∞–Ω–∏—á–Ω—ã–π –∫–µ–π—Å (–ë–ê–ì #27):

python
# –î–æ v17.9 ‚Äî –û–®–ò–ë–ö–ê:
# "–Ω–µ –±—ã–ª. –ò –≤–æ—Ç –Ω–µ–º—Ü—ã" ‚Üí "–Ω–µ –Ω–µ–º—Ü—ã" (—É–¥–∞–ª–µ–Ω–æ "–±—ã–ª.")
# N-–≥—Ä–∞–º–º–∞ "–±—ã–ª. –ò –≤–æ—Ç" ‚Üí sim=0.80 —Å —è–∫–æ—Ä–µ–º "–±—ã–ª–æ. –∏, –≤" ‚Üí –ª–æ–∂–Ω—ã–π loop

# –ü–æ—Å–ª–µ v17.9 ‚Äî –ü–†–ê–í–ò–õ–¨–ù–û:
# "–±—ã–ª. –ò –≤–æ—Ç" ‚Üí _count_meaningful=0 ‚Üí –≤ seen[] –Ω–µ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è ‚Üí –Ω–µ —É–¥–∞–ª—è–µ—Ç—Å—è
–°–∏–º—É–ª—è—Ü–∏—è: tests/simulations/sim_bug27_clean_loops.py

merge_replicas(segments: List[Dict]) -> List[Dict]
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–µ —Ä–µ–ø–ª–∏–∫–∏ –æ–¥–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞

–£—Å–ª–æ–≤–∏—è —Å–ª–∏—è–Ω–∏—è:

python
same_speaker and
pause < 3.0 and
same_raw_speaker_id and   # v16.0+
total_duration < 60
deduplicator.py
remove_cross_speaker_text_duplicates(segments: List[Dict]) -> List[Dict]
–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã —Ç–µ–∫—Å—Ç–∞ –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–∏–∫–µ—Ä–∞–º–∏

python
# –î–æ:
–ñ—É—Ä–Ω–∞–ª–∏—Å—Ç: "–î–∞-–¥–∞, –ø–æ–Ω—è—Ç–Ω–æ, –¥–∞"
–≠–∫—Å–ø–µ—Ä—Ç:   "–î–∞, –ø–æ–Ω—è—Ç–Ω–æ"  ‚Üê –¥—É–±–ª–∏–∫–∞—Ç

# –ü–æ—Å–ª–µ:
–ñ—É—Ä–Ω–∞–ª–∏—Å—Ç: "–î–∞-–¥–∞, –ø–æ–Ω—è—Ç–Ω–æ, –¥–∞"
–≠–∫—Å–ø–µ—Ä—Ç:   ""  ‚Üê —É–¥–∞–ª—ë–Ω
deduplicate_segments(segments: List[Dict]) -> List[Dict]
–£–¥–∞–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã

validator.py
validate_adjacent_same_speaker(segments: List[Dict]) -> List[Tuple[int, int]]
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–º–µ–∂–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã –æ–¥–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞

python
errors = validate_adjacent_same_speaker(segments)
# ‚Üí [(5, 6), (12, 13)]
auto_merge_adjacent_same_speaker(segments: List[Dict]) -> List[Dict]
üîß v16.0: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–ª–∏–≤–∞–µ—Ç —Å–º–µ–∂–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã

python
if (same_speaker and
    same_raw_speaker_id and   # ‚Üê –∑–∞—â–∏—Ç–∞ –æ—Ç –Ω–µ–≤–µ—Ä–Ω–æ–≥–æ —Å–ª–∏—è–Ω–∏—è
    pause < 3.0):
    merge()
generate_validation_report(segments: List[Dict], speaker_surname: str) -> Dict
–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:

python
{
    "total_segments": 241,
    "speakers": {
        "–ñ—É—Ä–Ω–∞–ª–∏—Å—Ç": 152,
        "–≠–∫—Å–ø–µ—Ä—Ç":   89
    },
    "total_duration": "01:53:47",
    "avg_segment_duration": {
        "–ñ—É—Ä–Ω–∞–ª–∏—Å—Ç": 5.2,
        "–≠–∫—Å–ø–µ—Ä—Ç":   18.7
    },
    "errors_found": 0,
    "warnings": ["Long pause at 00:15:32 (15s)"]
}
üìÇ scripts/export/ - –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö
json_export.py
export_to_json(json_path, segments_raw, segments_merged, file_info, speaker_roles, validation_report, corrections_log)
–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ JSON

txt_export.py
export_to_txt(txt_path: Path, segments: List[Dict])
–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Å–µ–≥–º–µ–Ω—Ç—ã –≤ TXT

–§–æ—Ä–º–∞—Ç:

text
[HH:MM:SS.mmm] –°–ø–∏–∫–µ—Ä: –¢–µ–∫—Å—Ç
jsons_to_txt(json_files: List[Path], txt_path: Path, speaker_surname: str)
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ JSON –≤ –æ–¥–∏–Ω TXT

–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: 2026-02-20