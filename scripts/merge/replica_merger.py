"""
merge/replica_merger.py - Ğ¡ĞºĞ»ĞµĞ¹ĞºĞ° Ñ€ĞµĞ¿Ğ»Ğ¸Ğº Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¿Ğ¸ĞºĞµÑ€Ğ°

ğŸ†• v17.10: FIX Ğ‘ĞĞ“ #32 - GAP_FILLED corruption + time-overlap duplicates
           GUARD A: Ğ¸Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ timestamp (end < start) â†’ DROP
           GUARD B: start < last_gap_end (Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ GAP Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½) â†’ DROP
           GUARD C: GAP Ñ â‰¥2 Ğ¾Ğ±Ñ‰Ğ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼Ğ¸ Ñ lookBEHIND â†’ DROP (corrupted)
                    GAP Ñ 0-1 Ğ¾Ğ±Ñ‰Ğ¸Ñ… ÑĞ»Ğ¾Ğ² â†’ KEEP (Ğ»ĞµĞ³Ğ¸Ñ‚Ğ¸Ğ¼Ğ½Ñ‹Ğ¹)
ğŸ†• v17.9: FIX Ğ‘ĞĞ“ #27 - Ğ›Ğ¾Ğ¶Ğ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ² Ñ low-meaningful N-Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ğ¼Ğ¸
ğŸ†• v17.4: FIX Ğ‘ĞĞ“ #17 - Ğ”ÑƒĞ±Ğ»Ğ¸ ÑĞ»Ğ¾Ğ² Ğ½Ğ° ÑÑ‚Ñ‹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¸ ÑĞºĞ»ĞµĞ¹ĞºĞµ
ğŸ†• v16.22: FIX Ğ‘ĞĞ“ #3 - Loop artifacts Ñ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ ÑĞ»Ğ¾Ğ²
ğŸ†• v16.21: CRITICAL FIX - Infinite Loop Ğ² overlap handling
ğŸ†• v16.20: DEBUG OUTPUT Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ°Ğ½Ğ¸Ñ
ğŸ†• v16.14: ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ FIX - speaker Ğ¾Ñ‚ Ğ¡ĞĞœĞĞ“Ğ Ğ”Ğ›Ğ˜ĞĞĞĞ“Ğ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°
"""

from difflib import SequenceMatcher
import re
from core.utils import seconds_to_hms
from corrections.hallucinations import clean_hallucinations_from_text
from merge.deduplicator import join_texts_deduplicated

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• v17.9: FIX Ğ‘ĞĞ“ #27 â€” ÑÑ‚Ğ¾Ğ¿-ÑĞ»Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµÑÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… N-Ğ³Ñ€Ğ°Ğ¼Ğ¼
# ROOT CAUSE: clean_loops ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ» N-Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ¸Ğ· Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ»Ğ¾Ğ²
# ("Ğ±Ñ‹Ğ». Ğ˜ Ğ²Ğ¾Ñ‚" â‰ˆ "Ğ±Ñ‹Ğ»Ğ¾. Ğ¸, Ğ²", sim=0.80) Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ğ¾ ÑƒĞ´Ğ°Ğ»ÑĞ» "Ğ±Ñ‹Ğ»."
# FIX: N-Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ñ < MIN_MEANINGFUL_WORDS Ğ·Ğ½Ğ°Ğ¼ĞµĞ½Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ² â€” Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼,
# Ğ½Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² seen[] Ğ¸ Ğ½Ğµ ÑƒĞ´Ğ°Ğ»ÑĞµĞ¼.
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RUSSIAN_STOP_WORDS = {
    # Ğ¤Ğ¾Ñ€Ğ¼Ñ‹ "Ğ±Ñ‹Ñ‚ÑŒ" â€” ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ´Ğ»Ñ Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ğ³Ğ°
    'Ğ±Ñ‹Ğ»', 'Ğ±Ñ‹Ğ»Ğ°', 'Ğ±Ñ‹Ğ»Ğ¾', 'Ğ±Ñ‹Ğ»Ğ¸', 'Ğ±ÑƒĞ´Ñƒ', 'Ğ±ÑƒĞ´ĞµÑ‚',
    'Ğ±ÑƒĞ´ÑƒÑ‚', 'Ğ±ÑƒĞ´ĞµĞ¼', 'Ğ±ÑƒĞ´ĞµÑ‚Ğµ', 'Ğ±Ñ‹Ğ²Ğ°ĞµÑ‚', 'ĞµÑÑ‚ÑŒ', 'Ğ±Ñ‹Ñ‚ÑŒ',
    # ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ³Ğ¸
    'Ğ²', 'Ğ²Ğ¾', 'Ğ½Ğ°', 'Ñ', 'ÑĞ¾', 'Ğº', 'ĞºĞ¾', 'Ğ¿Ğ¾', 'Ğ¸Ğ·',
    'Ğ·Ğ°', 'Ğ´Ğ¾', 'Ğ¿Ñ€Ğ¸', 'Ñ‡ĞµÑ€ĞµĞ·', 'Ğ¾Ğ±', 'Ğ¾', 'Ñƒ', 'Ğ´Ğ»Ñ',
    'Ğ¾Ñ‚', 'Ğ¿Ğ¾Ğ´', 'Ğ½Ğ°Ğ´', 'Ğ¿Ñ€Ğ¾', 'Ğ±ĞµĞ·', 'Ğ¼ĞµĞ¶Ğ´Ñƒ', 'ÑÑ€ĞµĞ´Ğ¸',
    # Ğ¡Ğ¾ÑĞ·Ñ‹
    'Ğ¸', 'Ğ°', 'Ğ½Ğ¾', 'Ğ¸Ğ»Ğ¸', 'Ñ‡Ñ‚Ğ¾', 'ĞºĞ°Ğº', 'ĞµÑĞ»Ğ¸', 'ĞºĞ¾Ğ³Ğ´Ğ°',
    'Ğ³Ğ´Ğµ', 'Ñ‡Ñ‚Ğ¾Ğ±Ñ‹', 'Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼Ñƒ', 'Ñ‚Ğ¾Ğ¶Ğµ', 'Ñ‚Ğ°ĞºĞ¶Ğµ', 'Ğ»Ğ¸Ğ±Ğ¾',
    'Ğ½Ğ¸', 'Ñ…Ğ¾Ñ‚Ñ', 'Ğ·Ğ°Ñ‚Ğ¾', 'Ğ¾Ğ´Ğ½Ğ°ĞºĞ¾',
    # Ğ§Ğ°ÑÑ‚Ğ¸Ñ†Ñ‹
    'Ğ²Ğ¾Ñ‚', 'Ğ¶Ğµ', 'Ğ»Ğ¸', 'Ğ±Ñ‹', 'Ğ½Ñƒ', 'Ğ½Ğµ', 'Ğ´Ğ°', 'Ñ‚Ğ¾', 'Ñ‚Ğ°Ğº',
    'Ğ»Ğ¸ÑˆÑŒ', 'Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾', 'Ğ´Ğ°Ğ¶Ğµ', 'ÑƒĞ¶',
    # Ğ›Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑÑ‚Ğ¾Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ (Ğ²ÑĞµ Ğ¿Ğ°Ğ´ĞµĞ¶Ğ¸)
    'Ñ', 'Ñ‚Ñ‹', 'Ğ¾Ğ½', 'Ğ¾Ğ½Ğ°', 'Ğ¾Ğ½Ğ¾', 'Ğ¼Ñ‹', 'Ğ²Ñ‹', 'Ğ¾Ğ½Ğ¸',
    'Ğ¼ĞµĞ½Ñ', 'Ñ‚ĞµĞ±Ñ', 'ĞµĞ³Ğ¾', 'ĞµÑ‘', 'ĞµĞµ', 'Ğ½Ğ°Ñ', 'Ğ²Ğ°Ñ', 'Ğ¸Ñ…',
    'Ğ¼Ğ½Ğµ', 'Ñ‚ĞµĞ±Ğµ', 'ĞµĞ¼Ñƒ', 'ĞµĞ¹', 'Ğ½Ğ°Ğ¼', 'Ğ²Ğ°Ğ¼', 'Ğ¸Ğ¼',
    # Ğ£ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑÑ‚Ğ¾Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ "ÑÑ‚Ğ¾"-Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°)
    'ÑÑ‚Ğ¾', 'ÑÑ‚Ğ¾Ñ‚', 'ÑÑ‚Ğ°', 'ÑÑ‚Ğ¸',
    # ĞĞ°Ñ€ĞµÑ‡Ğ¸Ñ Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²ĞµÑĞ¾Ğ¼
    'Ñ‚Ğ°Ğ¼', 'Ñ‚ÑƒÑ‚', 'Ğ·Ğ´ĞµÑÑŒ', 'Ñ‚Ğ¾Ğ³Ğ´Ğ°', 'ÑƒĞ¶Ğµ', 'ĞµÑ‰Ğµ', 'ĞµÑ‰Ñ‘',
    'Ğ¾Ñ‡ĞµĞ½ÑŒ', 'ÑĞ¾Ğ²ÑĞµĞ¼', 'Ğ²ĞµÑÑŒĞ¼Ğ°',
}
MIN_MEANINGFUL_WORDS = 2

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• v17.10: GUARD C â€” ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ‘ĞĞ“ #32
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

GAP_LOOKBEHIND_SEGS      = 5     # Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° lookBEHIND (Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ N Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²)
GAP_CORRUPTION_NGRAM_MIN = 3     # Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… ÑĞ»Ğ¾Ğ² Ğ² N-Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğµ
GAP_CORRUPTION_SIM       = 0.85  # Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ñ„Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ

def _count_meaningful(phrase: str) -> int:
    """
    Ğ¡Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ½Ğ°Ğ¼ĞµĞ½Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ² Ğ² N-Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğµ.
    ĞÑ‡Ğ¸Ñ‰Ğ°ĞµÑ‚ Ğ¿ÑƒĞ½ĞºÑ‚ÑƒĞ°Ñ†Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½Ğ¸Ğ¶Ğ½ĞµĞ¼Ñƒ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ñƒ.
    Ğ—Ğ½Ğ°Ğ¼ĞµĞ½Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ»Ğ¾Ğ²Ğ¾ = Ğ½Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² RUSSIAN_STOP_WORDS.
    """
    clean = re.sub(r'[.,!?;:Â«Â»"\'()\[\]]', '', phrase.lower())
    words = clean.split()
    return sum(1 for w in words if w not in RUSSIAN_STOP_WORDS)


def _meaningful_words(text: str) -> list:
    """
    ğŸ†• v17.10: Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… ÑĞ»Ğ¾Ğ² (len>3, Ğ½Ğµ ÑÑ‚Ğ¾Ğ¿-ÑĞ»Ğ¾Ğ²Ğ¾).
    Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ GUARD C Ğ´Ğ»Ñ lookBEHIND Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°.
    """
    clean = re.sub(r'[.,!?;:Â«Â»"\'()\[\]]', '', text.lower())
    return [w for w in clean.split() if w not in RUSSIAN_STOP_WORDS and len(w) > 3]

def _gap_is_corrupted(gap_text: str, preceding_texts: list, debug: bool = False) -> bool:
    gap_words    = _meaningful_words(gap_text)
    behind_words = _meaningful_words(' '.join(preceding_texts[-GAP_LOOKBEHIND_SEGS:]))
    if len(gap_words) < GAP_CORRUPTION_NGRAM_MIN or \
       len(behind_words) < GAP_CORRUPTION_NGRAM_MIN:
        if debug:
            print(f"    ğŸ” GUARD C: Ğ¼Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… ÑĞ»Ğ¾Ğ² â†’ KEEP")
        return False
    best_sim = 0.0
    best_ngram = ("", "")
    k_max = min(len(gap_words), len(behind_words), 6)
    for k in range(k_max, GAP_CORRUPTION_NGRAM_MIN - 1, -1):
        for gi in range(len(gap_words) - k + 1):
            gw = gap_words[gi:gi+k]
            for bi in range(len(behind_words) - k + 1):
                bw = behind_words[bi:bi+k]
                sim = SequenceMatcher(None, ' '.join(gw), ' '.join(bw)).ratio()
                if sim > best_sim:
                    best_sim = sim
                    best_ngram = (' '.join(gw), ' '.join(bw))
    if debug:
        print(f"    ğŸ” GUARD C ngram: best_sim={best_sim:.2f} "
              f"'{best_ngram[0]}' â‰ˆ '{best_ngram[1]}'")
    return best_sim >= GAP_CORRUPTION_SIM

def clean_loops(text, debug=False):
    """
    ğŸ†• v17.9: FIX Ğ‘ĞĞ“ #27 - ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞº N-Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ±ĞµĞ· Ğ·Ğ½Ğ°Ğ¼ĞµĞ½Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ²
    ğŸ†• v17.3: FIX Ğ‘ĞĞ“ #16 (v17.2) - Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ LOOP_WINDOW Ğ´Ğ¾ 30
    ğŸ†• v17.2: FIX Ğ‘ĞĞ“ #16 - ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¾ĞºĞ½Ğ° seen[] Ğ´Ğ»Ñ loop detection
    ğŸ†• v16.22: FIX Ğ‘ĞĞ“ #3 - Ğ”ĞµÑ‚ĞµĞºÑ†Ğ¸Ñ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ñ fuzzy matching
    ğŸ”§ v16.1: Ğ£Ğ´Ğ°Ğ»ÑĞµÑ‚ Ğ·Ğ°Ñ†Ğ¸ĞºĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ñ€Ğ°Ğ·Ñ‹ (loop artifacts)
    ğŸ†• v16.20: Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½ debug Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€
    """
    LOOP_WINDOW = 30

    if debug:
        print(f"    ğŸ§¹ clean_loops: Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‚ĞµĞºÑÑ‚Ğ° ({len(text)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ², {len(text.split())} ÑĞ»Ğ¾Ğ²)")

    words = text.split()
    seen = []
    cleaned = []
    removed_count = 0
    i = 0

    while i < len(words):
        phrase = ' '.join(words[i:i+3])
        phrase_lower = phrase.lower()

        # ğŸ†• v17.9: FIX Ğ‘ĞĞ“ #27 - Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞº N-Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ±ĞµĞ· Ğ·Ğ½Ğ°Ğ¼ĞµĞ½Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ²
        if _count_meaningful(phrase_lower) < MIN_MEANINGFUL_WORDS:
            cleaned.extend(words[i:i+3])
            i += 3
            continue

        is_loop = False
        for prev_phrase in seen:
            similarity = SequenceMatcher(None, phrase_lower, prev_phrase).ratio()
            if similarity >= 0.75:
                is_loop = True
                removed_count += 1
                if debug:
                    print(f"      ğŸ” LOOP (similarity={similarity:.2f}): '{phrase}' â‰ˆ '{prev_phrase}'")
                break

        if is_loop:
            if debug:
                left_context = ' '.join(cleaned[-3:]) if len(cleaned) >= 3 else ' '.join(cleaned)
                last_cleaned = cleaned[-1] if cleaned else "(Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾)"
                last_word    = last_cleaned.lower().rstrip('.,!?Â«Â»')
                HANGING_PREPOSITIONS = {
                    'Ğ½Ğ°', 'Ğ²', 'Ğ²Ğ¾', 'Ñ', 'ÑĞ¾', 'Ğº', 'Ğ¿Ğ¾', 'Ğ¸Ğ·', 'Ğ·Ğ°', 'Ğ´Ğ¾',
                    'Ğ¿Ñ€Ğ¸', 'Ñ‡ĞµÑ€ĞµĞ·', 'Ğ¾', 'Ğ¾Ğ±', 'Ñƒ', 'Ğ´Ğ»Ñ', 'Ğ¾Ñ‚', 'Ğ¿Ğ¾Ğ´', 'Ğ½Ğ°Ğ´'
                }
                print(f"      âš ï¸ Ğ£Ğ”ĞĞ›Ğ¯Ğ•Ğœ: '{phrase}'")
                print(f"         ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°: ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ñ '{prev_phrase}' (sim={similarity:.2f})")
                print(f"         ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ ÑĞ»ĞµĞ²Ğ°: '...{left_context}'")
                print(f"         Ğ¡Ğ»Ğ¾Ğ²Ğ¾ Ğ¿ĞµÑ€ĞµĞ´ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸ĞµĞ¼: '{last_cleaned}'")
                if last_word in HANGING_PREPOSITIONS:
                    print(f"         ğŸ”´ Ğ Ğ˜Ğ¡Ğš ĞĞ‘Ğ Ğ£Ğ‘ĞšĞ! '{last_cleaned}' â€” Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ³ Ğ±ĞµĞ· Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ñ!")

            # â”€â”€ v17.10 FIX Ğ‘ĞĞ“ #15 Ğ Ğ•Ğ“Ğ Ğ•Ğ¡Ğ¡Ğ˜Ğ¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            last_word_check = cleaned[-1].lower().rstrip('.,!?Â«Â»') if cleaned else ""
            HANGING_PREPOSITIONS = {
                'Ğ½Ğ°', 'Ğ²', 'Ğ²Ğ¾', 'Ñ', 'ÑĞ¾', 'Ğº', 'Ğ¿Ğ¾', 'Ğ¸Ğ·', 'Ğ·Ğ°', 'Ğ´Ğ¾',
                'Ğ¿Ñ€Ğ¸', 'Ñ‡ĞµÑ€ĞµĞ·', 'Ğ¾', 'Ğ¾Ğ±', 'Ñƒ', 'Ğ´Ğ»Ñ', 'Ğ¾Ñ‚', 'Ğ¿Ğ¾Ğ´', 'Ğ½Ğ°Ğ´'
            }
            if last_word_check in HANGING_PREPOSITIONS:
                if debug:
                    print(f"         ğŸ›¡ï¸ Ğ—ĞĞ©Ğ˜Ğ¢Ğ: Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ³ '{last_word_check}' â†’ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ")
                seen.append(phrase_lower)
                if len(seen) > LOOP_WINDOW:
                    seen.pop(0)
                cleaned.extend(words[i:i+3])
                i += 3
                continue
            # â”€â”€ ĞºĞ¾Ğ½ĞµÑ† FIX â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

            i += 1
            continue

        seen.append(phrase_lower)
        if len(seen) > LOOP_WINDOW:
            seen.pop(0)
        cleaned.extend(words[i:i+3])
        i += 3

    final = ' '.join(cleaned)
    final = re.sub(r'([.,!?])\1{2,}', r'\1', final)

    if debug:
        if removed_count > 0:
            print(f"    âœ… clean_loops: Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¾ ({len(final)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ², ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¾ {removed_count} loop artifacts)")
        else:
            print(f"    âœ… clean_loops: Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¾ ({len(final)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ², loops Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹)")

    return final.strip()


def merge_replicas(segments, debug=False):
    """
    ğŸ†• v17.10: FIX Ğ‘ĞĞ“ #32 â€” Ñ‚Ñ€Ğ¸ GUARD Ğ´Ğ»Ñ GAP_FILLED ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²
               GUARD A: end < start â†’ DROP (Ğ¸Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ timestamp)
               GUARD B: start < last_gap_end â†’ DROP (Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ)
               GUARD C: GAP corrupted Ğ¿Ğ¾ lookBEHIND â†’ DROP
    ğŸ†• v17.4: FIX Ğ‘ĞĞ“ #17 - join_texts_deduplicated Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ' '.join
    ğŸ†• v16.28.2: Ğ”Ğ•Ğ¢ĞĞ›Ğ¬ĞĞ«Ğ™ DEBUG Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°
    ğŸ†• v16.23: ĞĞ¡Ğ›ĞĞ‘Ğ›Ğ•ĞĞ˜Ğ• Ğ—ĞĞ©Ğ˜Ğ¢Ğ« v16.0 Ğ´Ğ»Ñ Ğ‘ĞĞ“ #4
    ğŸ†• v16.21: CRITICAL FIX - Infinite Loop Ğ² overlap handling
    """
    if not segments:
        return []

    def similarity(a, b):
        return SequenceMatcher(None, a.lower(), b.lower()).ratio() > 0.75

    merged = []
    i = 0
    merge_count = 0
    TARGET_RANGE = (590, 600)

    while i < len(segments):
        merge_count += 1
        current = segments[i]
        current_speaker = current['speaker']
        current_raw_id = current.get('raw_speaker_id', '')

        texts = [current['text']]
        current_end = current['end']
        start_time = current['start']
        all_segments_in_group = [current]

        # ğŸ†• v17.10: ÑĞ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ĞµĞ¼ last_gap_end Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ¸
        last_gap_end = None

        in_target_range = (start_time <= TARGET_RANGE[1] and current_end >= TARGET_RANGE[0])

        if debug or in_target_range:
            print(f"\n  ğŸ”€ MERGE #{merge_count}: {current.get('start_hms', seconds_to_hms(start_time))} {current_speaker} â€” Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾")
            if in_target_range:
                print(f"     ğŸ¯ TARGET RANGE Ğ‘ĞĞ“ #17 DETECTED! (Ğ¸Ñ‰ĞµĞ¼ 00:09:54)")
        else:
            print(f"  ğŸ”€ {current.get('start_hms', seconds_to_hms(start_time))} {current_speaker} â€” Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾ merge")

        if in_target_range:
            print(f"     ğŸ“ Ğ¡ĞµĞ³Ğ¼ĞµĞ½Ñ‚ #0: [{seconds_to_hms(current['start'])}-{seconds_to_hms(current['end'])}]")
            print(f"        Ğ¢ĞµĞºÑÑ‚: \"{current['text'][:80]}...\"")

        max_iterations = len(segments) * 2
        iteration_count = 0
        j = i + 1
        merge_continue = True
        segment_index = 1

        while j < len(segments) and merge_continue:
            iteration_count += 1
            if iteration_count > max_iterations:
                print(f"    âš ï¸ Ğ—ĞĞ©Ğ˜Ğ¢Ğ: Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞµĞ½Ğ¾ {max_iterations} Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° merge #{merge_count}")
                break

            next_seg = segments[j]
            next_raw_id = next_seg.get('raw_speaker_id', '')
            pause = next_seg['start'] - current_end

            # â”€â”€ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ Ğ¡ĞŸĞ˜ĞšĞ•Ğ Ğ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if next_seg['speaker'] != current_speaker:
                merge_continue = False
                break

            if current_raw_id != next_raw_id and current_raw_id and next_raw_id:
                if current_speaker not in ("Ğ–ÑƒÑ€Ğ½Ğ°Ğ»Ğ¸ÑÑ‚", "ĞĞ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€"):
                    print(f"    â†³ âš ï¸ raw_speaker_id Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ({current_raw_id} vs {next_raw_id}), Ğ½Ğ¾ speaker={current_speaker} â†’ âœ… merge anyway")
                else:
                    print(f"    â†³ âŒ raw_speaker_id Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ({current_raw_id} vs {next_raw_id}) Ğ´Ğ»Ñ {current_speaker} â†’ STOP")
                    merge_continue = False
                    break

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ğŸ†• v17.10: GUARD A â€” Ğ¸Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ timestamp
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if next_seg['end'] < next_seg['start']:
                print(f"    â†³ â›” GUARD A: [{next_seg['start']:.2f}â€“{next_seg['end']:.2f}] Ğ¸Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ â†’ DROP '{next_seg['text'][:40]}'")
                current_end = max(current_end, next_seg['end'])
                j += 1
                continue

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ğŸ†• v17.10: GUARD B â€” Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ñ GAP Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ¾Ğ¼
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if last_gap_end is not None and next_seg['start'] < last_gap_end:
                print(f"    â†³ â›” GUARD B: start({next_seg['start']:.2f}) < gap_end({last_gap_end:.2f}) â†’ DROP '{next_seg['text'][:40]}'")
                current_end = max(current_end, next_seg['end'])
                j += 1
                continue

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ğŸ†• v17.10: GUARD C â€” GAP_FILLED corruption check
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if next_seg.get('from_gap'):
                last_gap_end = next_seg['end']   # Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ĞµĞ¼ Ğ’Ğ¡Ğ•Ğ“Ğ”Ğ
                if _gap_is_corrupted(next_seg['text'], texts, debug=debug):
                    print(f"    â†³ â›” GUARD C: GAP corrupted â†’ DROP '{next_seg['text'][:50]}'")
                    current_end = max(current_end, next_seg['end'])
                    j += 1
                    continue
                else:
                    print(f"    â†³ âœ… GUARD C: GAP Ğ»ĞµĞ³Ğ¸Ñ‚Ğ¸Ğ¼ĞµĞ½ â†’ KEEP '{next_seg['text'][:50]}'")
                    # Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼ Ğ² ÑˆÑ‚Ğ°Ñ‚Ğ½ÑƒÑ pause-Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ

            # â”€â”€ Ğ¨Ğ¢ĞĞ¢ĞĞĞ¯ PAUSE-Ğ›ĞĞ“Ğ˜ĞšĞ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if pause < 0:
                sim = SequenceMatcher(None, texts[-1] if texts else "", next_seg['text']).ratio()

                if sim > 0.85:
                    if len(next_seg['text']) > len(texts[-1]):
                        texts[-1] = next_seg['text']
                        all_segments_in_group[-1] = next_seg
                        if in_target_range:
                            print(f"     ğŸ”„ Ğ¡ĞµĞ³Ğ¼ĞµĞ½Ñ‚ #{segment_index-1} Ğ—ĞĞœĞ•ĞĞĞ (Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚, Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¹)")
                    current_end = next_seg['end']
                    j += 1
                    continue

                if sim > 0.60:
                    texts.append(next_seg['text'])
                    all_segments_in_group.append(next_seg)
                    if in_target_range:
                        print(f"     â• Ğ¡ĞµĞ³Ğ¼ĞµĞ½Ñ‚ #{segment_index}: (overlap, sim={sim:.2f}) \"{next_seg['text'][:80]}\"")
                    segment_index += 1
                    current_end = next_seg['end']
                    j += 1
                    continue

                if abs(pause) <= 2.0:
                    texts.append(next_seg['text'])
                    all_segments_in_group.append(next_seg)
                    if in_target_range:
                        print(f"     â• Ğ¡ĞµĞ³Ğ¼ĞµĞ½Ñ‚ #{segment_index}: (overlap {abs(pause):.1f}s) \"{next_seg['text'][:80]}\"")
                    segment_index += 1
                    current_end = next_seg['end']
                    j += 1
                    continue
                else:
                    print(f"    â†³ Overlap {abs(pause):.1f}s Ğ±ĞµĞ· similarity â†’ âŒ STOP")
                    merge_continue = False
                    break

            else:
                if current_speaker != "Ğ–ÑƒÑ€Ğ½Ğ°Ğ»Ğ¸ÑÑ‚":
                    if pause <= 2.0:
                        texts.append(next_seg['text'])
                        all_segments_in_group.append(next_seg)
                        if in_target_range:
                            print(f"     â• Ğ¡ĞµĞ³Ğ¼ĞµĞ½Ñ‚ #{segment_index}: (Ğ¿Ğ°ÑƒĞ·Ğ° {pause:.1f}s) \"{next_seg['text'][:80]}\"")
                        else:
                            print(f"    â†³ {next_seg.get('start_hms', '')} â¸ï¸ {pause:.1f}s â†’ âœ… merge")
                        segment_index += 1
                        current_end = next_seg['end']
                        j += 1
                        continue
                    elif pause <= 5.0 and any(similarity(next_seg['text'], t) for t in texts[-2:]):
                        texts.append(next_seg['text'])
                        all_segments_in_group.append(next_seg)
                        if in_target_range:
                            print(f"     â• Ğ¡ĞµĞ³Ğ¼ĞµĞ½Ñ‚ #{segment_index}: (Ğ¿Ğ°ÑƒĞ·Ğ° {pause:.1f}s, similarity) \"{next_seg['text'][:80]}\"")
                        else:
                            print(f"    â†³ {next_seg.get('start_hms', '')} â¸ï¸ {pause:.1f}s â†’ âœ… merge (similarity)")
                        segment_index += 1
                        current_end = next_seg['end']
                        j += 1
                        continue
                    else:
                        print(f"    â†³ {current_speaker} Ğ¿Ğ°ÑƒĞ·Ğ° {pause:.1f}s > 5.0s â†’ âŒ STOP")
                        merge_continue = False
                        break

                if current_speaker == "Ğ–ÑƒÑ€Ğ½Ğ°Ğ»Ğ¸ÑÑ‚":
                    if pause <= 3.0 and pause >= -12.0:
                        texts.append(next_seg['text'])
                        all_segments_in_group.append(next_seg)
                        print(f"    â†³ {next_seg.get('start_hms', '')} â¸ï¸ {pause:.1f}s â†’ âœ… merge")
                        segment_index += 1
                        current_end = next_seg['end']
                        j += 1
                        continue
                    else:
                        print(f"    â†³ Ğ–ÑƒÑ€Ğ½Ğ°Ğ»Ğ¸ÑÑ‚ Ğ¿Ğ°ÑƒĞ·Ğ° {pause:.1f}s > 3.0s â†’ âŒ STOP")
                        merge_continue = False
                        break

        total_words = sum(len(t.split()) for t in texts)
        if in_target_range:
            print(f"\n     ğŸ“Š Ğ’ÑĞµĞ³Ğ¾ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ¾: {len(texts)} ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², {total_words} ÑĞ»Ğ¾Ğ²")
            print(f"     ğŸ“Š Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½: [{seconds_to_hms(start_time)}-{seconds_to_hms(current_end)}]")
        elif debug:
            print(f"    ğŸ“Š Ğ¡Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ¾: {len(texts)} ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², {total_words} ÑĞ»Ğ¾Ğ²")

        dominant_segment = max(all_segments_in_group, key=lambda s: len(s.get('text', '')))

        if len(all_segments_in_group) > 1:
            print(f"    ğŸ¯ Ğ”Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹: {dominant_segment.get('speaker')} / {dominant_segment.get('raw_speaker_id')} (Ğ´Ğ»Ğ¸Ğ½Ğ°: {len(dominant_segment.get('text', ''))} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²)")

        if in_target_range or debug:
            print(f"    ğŸ”— Ğ’Ñ‹Ğ·Ğ¾Ğ² join_texts_deduplicated Ğ´Ğ»Ñ {len(texts)} Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²...")

        final_text = join_texts_deduplicated(texts, debug=(in_target_range or debug))

        if in_target_range:
            print(f"\n     ğŸ“ ĞŸĞ¾ÑĞ»Ğµ join_texts_deduplicated:")
            print(f"        \"{final_text[:100]}...\"")

        if debug or in_target_range:
            print(f"    ğŸ§¹ Ğ’Ñ‹Ğ·Ğ¾Ğ² clean_loops ({len(final_text)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²)...")

        final_text = clean_loops(final_text, debug=(debug or in_target_range))

        if debug or in_target_range:
            print(f"    ğŸ§¹ Ğ’Ñ‹Ğ·Ğ¾Ğ² clean_hallucinations_from_text...")

        final_text = clean_hallucinations_from_text(final_text, current_speaker, debug=(debug or in_target_range))

        if in_target_range:
            print(f"\n     âœ… Ğ¤Ğ˜ĞĞĞ›Ğ¬ĞĞ«Ğ™ Ñ‚ĞµĞºÑÑ‚ ({len(final_text)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²):")
            print(f"        \"{final_text[:100]}...\"")
            if "Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾" in final_text.lower():
                print(f"        âŒ Ğ”Ğ£Ğ‘Ğ›Ğ¬ Â«Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ”Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Â» Ğ’Ğ¡Ğ Ğ•Ğ©Ğ Ğ•Ğ¡Ğ¢Ğ¬!")
            else:
                print(f"        âœ… Ğ”ÑƒĞ±Ğ»ÑŒ Â«Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ”Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Â» Ğ£Ğ¡Ğ¢Ğ ĞĞĞĞ!")
        elif debug:
            print(f"    âœ… ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°, Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚: {len(final_text)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²")

        if final_text:
            merged.append({
                "speaker": dominant_segment.get('speaker', current_speaker),
                "time": current.get('start_hms', seconds_to_hms(start_time)),
                "start": start_time,
                "end": current_end,
                "text": final_text,
                "confidence": current.get('confidence', ''),
                "raw_speaker_id": dominant_segment.get('raw_speaker_id', ''),
                "sub_segments": [
                    {
                        "start": s["start"],
                        "end":   s["end"],
                        "words": len(s.get("text", "").split())
                    }
                    for s in all_segments_in_group
                ]
            })

            if len(texts) > 1:
                print(f"  âœ… Ğ¡ĞºĞ»ĞµĞµĞ½Ğ¾ {len(texts)} ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² â†’ {len(final_text.split())} ÑĞ»Ğ¾Ğ²")

        i = j

    if debug:
        print(f"\nâœ… merge_replicas Ğ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½: {len(merged)} merged ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· {len(segments)} Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ…")

    return merged
