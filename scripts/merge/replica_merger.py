"""
merge/replica_merger.py - –°–∫–ª–µ–π–∫–∞ —Ä–µ–ø–ª–∏–∫ –æ–¥–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞

üÜï v17.9: FIX –ë–ê–ì #27 - –õ–æ–∂–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —Å–ª–æ–≤ —Å low-meaningful N-–≥—Ä–∞–º–º–∞–º–∏
üÜï v17.4: FIX –ë–ê–ì #17 - –î—É–±–ª–∏ —Å–ª–æ–≤ –Ω–∞ —Å—Ç—ã–∫–∞—Ö –ø—Ä–∏ —Å–∫–ª–µ–π–∫–µ
üÜï v16.22: FIX –ë–ê–ì #3 - Loop artifacts —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏ —Å–ª–æ–≤
üÜï v16.21: CRITICAL FIX - Infinite Loop –≤ overlap handling
üÜï v16.20: DEBUG OUTPUT –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∑–∞–≤–∏—Å–∞–Ω–∏—è
üÜï v16.14: –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô FIX - speaker –æ—Ç –°–ê–ú–û–ì–û –î–õ–ò–ù–ù–û–ì–û —Å–µ–≥–º–µ–Ω—Ç–∞
"""

from difflib import SequenceMatcher
import re
from core.utils import seconds_to_hms
from corrections.hallucinations import clean_hallucinations_from_text
from merge.deduplicator import join_texts_deduplicated

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üÜï v17.9: FIX –ë–ê–ì #27 ‚Äî —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –±–µ—Å—Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã—Ö N-–≥—Ä–∞–º–º
# ROOT CAUSE: clean_loops —Å—Ä–∞–≤–Ω–∏–≤–∞–ª N-–≥—Ä–∞–º–º—ã –∏–∑ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ª–æ–≤
# ("–±—ã–ª. –ò –≤–æ—Ç" ‚âà "–±—ã–ª–æ. –∏, –≤", sim=0.80) –∏ –æ—à–∏–±–æ—á–Ω–æ —É–¥–∞–ª—è–ª "–±—ã–ª."
# FIX: N-–≥—Ä–∞–º–º—ã —Å < MIN_MEANINGFUL_WORDS –∑–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º,
# –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º –≤ seen[] –∏ –Ω–µ —É–¥–∞–ª—è–µ–º.
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

RUSSIAN_STOP_WORDS = {
    # –§–æ—Ä–º—ã "–±—ã—Ç—å" ‚Äî –∫–ª—é—á–µ–≤—ã–µ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –±–∞–≥–∞
    '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–æ', '–±—ã–ª–∏', '–±—É–¥—É', '–±—É–¥–µ—Ç',
    '–±—É–¥—É—Ç', '–±—É–¥–µ–º', '–±—É–¥–µ—Ç–µ', '–±—ã–≤–∞–µ—Ç', '–µ—Å—Ç—å', '–±—ã—Ç—å',
    # –ü—Ä–µ–¥–ª–æ–≥–∏
    '–≤', '–≤–æ', '–Ω–∞', '—Å', '—Å–æ', '–∫', '–∫–æ', '–ø–æ', '–∏–∑',
    '–∑–∞', '–¥–æ', '–ø—Ä–∏', '—á–µ—Ä–µ–∑', '–æ–±', '–æ', '—É', '–¥–ª—è',
    '–æ—Ç', '–ø–æ–¥', '–Ω–∞–¥', '–ø—Ä–æ', '–±–µ–∑', '–º–µ–∂–¥—É', '—Å—Ä–µ–¥–∏',
    # –°–æ—é–∑—ã
    '–∏', '–∞', '–Ω–æ', '–∏–ª–∏', '—á—Ç–æ', '–∫–∞–∫', '–µ—Å–ª–∏', '–∫–æ–≥–¥–∞',
    '–≥–¥–µ', '—á—Ç–æ–±—ã', '–ø–æ—Ç–æ–º—É', '—Ç–æ–∂–µ', '—Ç–∞–∫–∂–µ', '–ª–∏–±–æ',
    '–Ω–∏', '—Ö–æ—Ç—è', '–∑–∞—Ç–æ', '–æ–¥–Ω–∞–∫–æ',
    # –ß–∞—Å—Ç–∏—Ü—ã
    '–≤–æ—Ç', '–∂–µ', '–ª–∏', '–±—ã', '–Ω—É', '–Ω–µ', '–¥–∞', '—Ç–æ', '—Ç–∞–∫',
    '–ª–∏—à—å', '—Ç–æ–ª—å–∫–æ', '–¥–∞–∂–µ', '—É–∂',
    # –õ–∏—á–Ω—ã–µ –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è (–≤—Å–µ –ø–∞–¥–µ–∂–∏)
    '—è', '—Ç—ã', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–æ', '–º—ã', '–≤—ã', '–æ–Ω–∏',
    '–º–µ–Ω—è', '—Ç–µ–±—è', '–µ–≥–æ', '–µ—ë', '–µ–µ', '–Ω–∞—Å', '–≤–∞—Å', '–∏—Ö',
    '–º–Ω–µ', '—Ç–µ–±–µ', '–µ–º—É', '–µ–π', '–Ω–∞–º', '–≤–∞–º', '–∏–º',
    # –£–∫–∞–∑–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è (—Ç–æ–ª—å–∫–æ "—ç—Ç–æ"-–≥—Ä—É–ø–ø–∞)
    '—ç—Ç–æ', '—ç—Ç–æ—Ç', '—ç—Ç–∞', '—ç—Ç–∏',
    # –ù–∞—Ä–µ—á–∏—è —Å –Ω–∏–∑–∫–∏–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –≤–µ—Å–æ–º
    '—Ç–∞–º', '—Ç—É—Ç', '–∑–¥–µ—Å—å', '—Ç–æ–≥–¥–∞', '—É–∂–µ', '–µ—â–µ', '–µ—â—ë',
    '–æ—á–µ–Ω—å', '—Å–æ–≤—Å–µ–º', '–≤–µ—Å—å–º–∞',
}
MIN_MEANINGFUL_WORDS = 2


def _count_meaningful(phrase: str) -> int:
    """
    –°—á–∏—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –≤ N-–≥—Ä–∞–º–º–µ.
    –û—á–∏—â–∞–µ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É.
    –ó–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ = –Ω–µ –≤—Ö–æ–¥–∏—Ç –≤ RUSSIAN_STOP_WORDS.
    """
    clean = re.sub(r'[.,!?;:¬´¬ª"\'()\[\]]', '', phrase.lower())
    words = clean.split()
    return sum(1 for w in words if w not in RUSSIAN_STOP_WORDS)


def clean_loops(text, debug=False):
    """
    üÜï v17.9: FIX –ë–ê–ì #27 - –ü—Ä–æ–ø—É—Å–∫ N-–≥—Ä–∞–º–º –±–µ–∑ –∑–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤
    üÜï v17.3: FIX –ë–ê–ì #16 (v17.2) - –£–≤–µ–ª–∏—á–µ–Ω–∏–µ LOOP_WINDOW –¥–æ 30
    üÜï v17.2: FIX –ë–ê–ì #16 - –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –æ–∫–Ω–∞ seen[] –¥–ª—è loop detection
    üÜï v16.22: FIX –ë–ê–ì #3 - –î–µ—Ç–µ–∫—Ü–∏—è –≤–∞—Ä–∏–∞—Ü–∏–π —Å fuzzy matching
    üîß v16.1: –£–¥–∞–ª—è–µ—Ç –∑–∞—Ü–∏–∫–ª–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã (loop artifacts)
    üÜï v16.20: –î–æ–±–∞–≤–ª–µ–Ω debug –ø–∞—Ä–∞–º–µ—Ç—Ä

    **–ü–†–û–ë–õ–ï–ú–ê (–ë–ê–ì #27):**
    clean_loops —É–¥–∞–ª—è–ª —Å–ª–æ–≤–æ "–±—ã–ª." –∏–∑ —Ñ—Ä–∞–∑—ã "–Ω–æ –ñ—É–∫–æ–≤ –Ω–µ –±—ã–ª. –ò –≤–æ—Ç –Ω–µ–º—Ü—ã".
    N-–≥—Ä–∞–º–º–∞ "–±—ã–ª. –ò –≤–æ—Ç" —Å—Ä–∞–≤–Ω–∏–≤–∞–ª–∞—Å—å —Å —è–∫–æ—Ä–µ–º "–±—ã–ª–æ. –∏, –≤" (sim=0.80 ‚â• 0.75)
    –∏ –æ—à–∏–±–æ—á–Ω–æ —Å—á–∏—Ç–∞–ª–∞—Å—å loop artifact.

    **ROOT CAUSE:**
    N-–≥—Ä–∞–º–º—ã –∏–∑ —á–∏—Å—Ç–æ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ª–æ–≤ (—Ñ–æ—Ä–º—ã "–±—ã—Ç—å" + —Å–æ—é–∑—ã + —á–∞—Å—Ç–∏—Ü—ã)
    –¥–∞—é—Ç –≤—ã—Å–æ–∫–æ–µ fuzzy-—Å—Ö–æ–¥—Å—Ç–≤–æ –±–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–≤—è–∑–∏.
    –ü–æ—Ä–æ–≥ 0.75 –Ω–µ –∑–∞—â–∏—â–∞–ª –æ—Ç —Ç–∞–∫–∏—Ö –∫–æ–ª–ª–∏–∑–∏–π.

    **FIX v17.9:**
    –ü–µ—Ä–µ–¥ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º N-–≥—Ä–∞–º–º—ã –≤ seen[] –∏ –ø–µ—Ä–µ–¥ loop-–ø—Ä–æ–≤–µ—Ä–∫–æ–π:
    —Å—á–∏—Ç–∞–µ–º –∑–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞. –ï—Å–ª–∏ < MIN_MEANINGFUL_WORDS=2 ‚Äî
    —Å–ª–æ–≤–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ output, –Ω–æ –≤ seen[] –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º –∏ loop –Ω–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º.

    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
        debug: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å debug output

    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ loop artifacts
    """
    LOOP_WINDOW = 30

    if debug:
        print(f"    üßπ clean_loops: –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤, {len(text.split())} —Å–ª–æ–≤)")

    words = text.split()
    seen = []
    cleaned = []

    removed_count = 0
    i = 0

    while i < len(words):
        phrase = ' '.join(words[i:i+3])
        phrase_lower = phrase.lower()

        # üÜï v17.9: FIX –ë–ê–ì #27 - –ø—Ä–æ–ø—É—Å–∫ N-–≥—Ä–∞–º–º –±–µ–∑ –∑–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤
        if _count_meaningful(phrase_lower) < MIN_MEANINGFUL_WORDS:
            # –°–ª–æ–≤–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ output
            # –í seen[] –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º ‚Äî –Ω–µ —Å–æ–∑–¥–∞—ë–º –ª–æ–∂–Ω—ã—Ö —è–∫–æ—Ä–µ–π
            cleaned.extend(words[i:i+3])
            i += 3
            continue

        # Fuzzy matching —Ç–æ–ª—å–∫–æ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –æ–∫–Ω–∞
        is_loop = False

        for prev_phrase in seen:
            similarity = SequenceMatcher(None, phrase_lower, prev_phrase).ratio()

            if similarity >= 0.75:
                is_loop = True
                removed_count += 1

                if debug:
                    print(f"      üîÅ LOOP (similarity={similarity:.2f}): '{phrase}' ‚âà '{prev_phrase}'")

                break

        if is_loop:
            i += 1
            continue

        # üÜï v17.3: –î–æ–±–∞–≤–ª—è–µ–º —Ñ—Ä–∞–∑—É –≤ seen —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –æ–∫–Ω–∞
        seen.append(phrase_lower)
        if len(seen) > LOOP_WINDOW:
            seen.pop(0)

        cleaned.extend(words[i:i+3])
        i += 3

    final = ' '.join(cleaned)
    final = re.sub(r'([.,!?])\1{2,}', r'\1', final)

    if debug:
        if removed_count > 0:
            print(f"    ‚úÖ clean_loops: –≥–æ—Ç–æ–≤–æ ({len(final)} —Å–∏–º–≤–æ–ª–æ–≤, —É–¥–∞–ª–µ–Ω–æ {removed_count} loop artifacts)")
        else:
            print(f"    ‚úÖ clean_loops: –≥–æ—Ç–æ–≤–æ ({len(final)} —Å–∏–º–≤–æ–ª–æ–≤, loops –Ω–µ –Ω–∞–π–¥–µ–Ω—ã)")

    return final.strip()


def merge_replicas(segments, debug=False):
    """
    üÜï v17.4: FIX –ë–ê–ì #17 - join_texts_deduplicated –≤–º–µ—Å—Ç–æ ' '.join
    üÜï v16.28.2: –î–ï–¢–ê–õ–¨–ù–´–ô DEBUG –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ç–µ—Ä–∏ —Ç–µ–∫—Å—Ç–∞
    üÜï v16.23: –û–°–õ–ê–ë–õ–ï–ù–ò–ï –ó–ê–©–ò–¢–´ v16.0 –¥–ª—è –ë–ê–ì #4
    üÜï v16.21: CRITICAL FIX - Infinite Loop –≤ overlap handling
    ...
    """
    if not segments:
        return []

    def similarity(a, b):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç—ã –ø–æ—Ö–æ–∂–∏ > 75%"""
        return SequenceMatcher(None, a.lower(), b.lower()).ratio() > 0.75

    merged = []
    i = 0
    merge_count = 0

    # üÜï v17.4: –¶–µ–ª–µ–≤–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è –ë–ê–ì #17 (00:09:54 = 594 —Å–µ–∫—É–Ω–¥—ã)
    TARGET_RANGE = (590, 600)

    while i < len(segments):
        merge_count += 1
        current = segments[i]
        current_speaker = current['speaker']

        # üÜï v16.23: –ë–ê–ì #4 FIX - –±–µ—Ä—ë–º raw_speaker_id –¥–ª—è –∑–∞—â–∏—Ç—ã
        current_raw_id = current.get('raw_speaker_id', '')

        texts = [current['text']]
        current_end = current['end']
        start_time = current['start']

        # –°–æ–±–∏—Ä–∞–µ–º –í–°–ï —Å–µ–≥–º–µ–Ω—Ç—ã –≥—Ä—É–ø–ø—ã
        all_segments_in_group = [current]

        # üÜï v17.4: –ü—Ä–æ–≤–µ—Ä–∫–∞ - –ø–æ–ø–∞–¥–∞–µ—Ç –ª–∏ —ç—Ç–æ—Ç merge –≤ —Ü–µ–ª–µ–≤–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω –ë–ê–ì #17?
        in_target_range = (start_time <= TARGET_RANGE[1] and current_end >= TARGET_RANGE[0])

        if debug or in_target_range:
            print(f"\n  üîÄ MERGE #{merge_count}: {current.get('start_hms', seconds_to_hms(start_time))} {current_speaker} ‚Äî –Ω–∞—á–∞–ª–æ")
            if in_target_range:
                print(f"     üéØ TARGET RANGE –ë–ê–ì #17 DETECTED! (–∏—â–µ–º 00:09:54)")
        else:
            print(f"  üîÄ {current.get('start_hms', seconds_to_hms(start_time))} {current_speaker} ‚Äî –Ω–∞—á–∞–ª–æ merge")

        # üÜï v17.4: –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–π —Å–µ–≥–º–µ–Ω—Ç
        if in_target_range:
            print(f"     üìù –°–µ–≥–º–µ–Ω—Ç #0: [{seconds_to_hms(current['start'])}-{seconds_to_hms(current['end'])}]")
            print(f"        –¢–µ–∫—Å—Ç: \"{current['text'][:80]}...\"")

        # –ó–∞—â–∏—Ç–∞ –æ—Ç infinite loop
        max_iterations = len(segments) * 2
        iteration_count = 0

        # –ò—â–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã —Ç–æ–≥–æ –∂–µ —Å–ø–∏–∫–µ—Ä–∞
        j = i + 1
        merge_continue = True
        segment_index = 1

        while j < len(segments) and merge_continue:
            iteration_count += 1
            if iteration_count > max_iterations:
                print(f"    ‚ö†Ô∏è –ó–ê–©–ò–¢–ê: –ø—Ä–µ–≤—ã—à–µ–Ω–æ {max_iterations} –∏—Ç–µ—Ä–∞—Ü–∏–π –Ω–∞ merge #{merge_count}")
                break

            next_seg = segments[j]
            next_raw_id = next_seg.get('raw_speaker_id', '')
            pause = next_seg['start'] - current_end

            # üÜï v16.23: –û–°–õ–ê–ë–õ–ï–ù–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –°–ü–ò–ö–ï–†–ê –¥–ª—è –ë–ê–ì #4
            if next_seg['speaker'] != current_speaker:
                merge_continue = False
                break

            # üÜï v16.23: –ï—Å–ª–∏ speaker –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π, –Ω–æ raw_speaker_id —Ä–∞–∑–Ω—ã–µ
            if current_raw_id != next_raw_id and current_raw_id and next_raw_id:
                if current_speaker not in ("–ñ—É—Ä–Ω–∞–ª–∏—Å—Ç", "–û–ø–µ—Ä–∞—Ç–æ—Ä"):
                    print(f"    ‚Ü≥ ‚ö†Ô∏è raw_speaker_id —Ä–∞–∑–Ω—ã–µ ({current_raw_id} vs {next_raw_id}), –Ω–æ speaker={current_speaker} ‚Üí ‚úÖ merge anyway")
                else:
                    print(f"    ‚Ü≥ ‚ùå raw_speaker_id —Ä–∞–∑–Ω—ã–µ ({current_raw_id} vs {next_raw_id}) –¥–ª—è {current_speaker} ‚Üí STOP")
                    merge_continue = False
                    break

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ overlap (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è –ø–∞—É–∑–∞)
            if pause < 0:
                sim = SequenceMatcher(
                    None,
                    texts[-1] if texts else "",
                    next_seg['text']
                ).ratio()

                if sim > 0.85:
                    if len(next_seg['text']) > len(texts[-1]):
                        texts[-1] = next_seg['text']
                        all_segments_in_group[-1] = next_seg

                        if in_target_range:
                            print(f"     üîÑ –°–µ–≥–º–µ–Ω—Ç #{segment_index-1} –ó–ê–ú–ï–ù–Å–ù (–¥—É–±–ª–∏–∫–∞—Ç, –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–π)")
                            print(f"        –ù–æ–≤—ã–π —Ç–µ–∫—Å—Ç: \"{next_seg['text'][:80]}...\"")

                    current_end = next_seg['end']
                    j += 1
                    continue

                if sim > 0.60:
                    texts.append(next_seg['text'])
                    all_segments_in_group.append(next_seg)

                    if in_target_range:
                        print(f"     ‚ûï –°–µ–≥–º–µ–Ω—Ç #{segment_index}: [{seconds_to_hms(next_seg['start'])}-{seconds_to_hms(next_seg['end'])}] (overlap, sim={sim:.2f})")
                        print(f"        –¢–µ–∫—Å—Ç: \"{next_seg['text'][:80]}...\"")

                    segment_index += 1
                    current_end = next_seg['end']
                    j += 1
                    continue

                if abs(pause) <= 2.0:
                    texts.append(next_seg['text'])
                    all_segments_in_group.append(next_seg)

                    if in_target_range:
                        print(f"     ‚ûï –°–µ–≥–º–µ–Ω—Ç #{segment_index}: [{seconds_to_hms(next_seg['start'])}-{seconds_to_hms(next_seg['end'])}] (overlap {abs(pause):.1f}s)")
                        print(f"        –¢–µ–∫—Å—Ç: \"{next_seg['text'][:80]}...\"")

                    segment_index += 1
                    current_end = next_seg['end']
                    j += 1
                    continue
                else:
                    print(f"    ‚Ü≥ Overlap {abs(pause):.1f}s –±–µ–∑ similarity ‚Üí ‚ùå STOP")
                    merge_continue = False
                    break

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—ã—á–Ω–æ–π –ø–∞—É–∑—ã
            else:
                # –î–ª—è –ù–ï-–ñ—É—Ä–Ω–∞–ª–∏—Å—Ç–∞
                if current_speaker != "–ñ—É—Ä–Ω–∞–ª–∏—Å—Ç":
                    if pause <= 2.0:
                        texts.append(next_seg['text'])
                        all_segments_in_group.append(next_seg)

                        if in_target_range:
                            print(f"     ‚ûï –°–µ–≥–º–µ–Ω—Ç #{segment_index}: [{seconds_to_hms(next_seg['start'])}-{seconds_to_hms(next_seg['end'])}] (–ø–∞—É–∑–∞ {pause:.1f}s)")
                            print(f"        –¢–µ–∫—Å—Ç: \"{next_seg['text'][:80]}...\"")
                        else:
                            print(f"    ‚Ü≥ {next_seg.get('start_hms', '')} ‚è∏Ô∏è {pause:.1f}s ‚Üí ‚úÖ merge")

                        segment_index += 1
                        current_end = next_seg['end']
                        j += 1
                        continue
                    elif pause <= 5.0 and any(similarity(next_seg['text'], t) for t in texts[-2:]):
                        texts.append(next_seg['text'])
                        all_segments_in_group.append(next_seg)

                        if in_target_range:
                            print(f"     ‚ûï –°–µ–≥–º–µ–Ω—Ç #{segment_index}: [{seconds_to_hms(next_seg['start'])}-{seconds_to_hms(next_seg['end'])}] (–ø–∞—É–∑–∞ {pause:.1f}s, similarity)")
                            print(f"        –¢–µ–∫—Å—Ç: \"{next_seg['text'][:80]}...\"")
                        else:
                            print(f"    ‚Ü≥ {next_seg.get('start_hms', '')} ‚è∏Ô∏è {pause:.1f}s ‚Üí ‚úÖ merge (similarity)")

                        segment_index += 1
                        current_end = next_seg['end']
                        j += 1
                        continue
                    else:
                        print(f"    ‚Ü≥ {current_speaker} –ø–∞—É–∑–∞ {pause:.1f}s > 5.0s ‚Üí ‚ùå STOP")
                        merge_continue = False
                        break

                # –î–ª—è –ñ—É—Ä–Ω–∞–ª–∏—Å—Ç–∞
                if current_speaker == "–ñ—É—Ä–Ω–∞–ª–∏—Å—Ç":
                    if pause <= 3.0 and pause >= -12.0:
                        texts.append(next_seg['text'])
                        all_segments_in_group.append(next_seg)
                        print(f"    ‚Ü≥ {next_seg.get('start_hms', '')} ‚è∏Ô∏è {pause:.1f}s ‚Üí ‚úÖ merge")
                        segment_index += 1
                        current_end = next_seg['end']
                        j += 1
                        continue
                    else:
                        print(f"    ‚Ü≥ –ñ—É—Ä–Ω–∞–ª–∏—Å—Ç –ø–∞—É–∑–∞ {pause:.1f}s > 3.0s ‚Üí ‚ùå STOP")
                        merge_continue = False
                        break

        # üÜï v17.4: –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∫–ª–µ–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        total_words = sum(len(t.split()) for t in texts)
        if in_target_range:
            print(f"\n     üìä –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(texts)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤, {total_words} —Å–ª–æ–≤")
            print(f"     üìä –§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω: [{seconds_to_hms(start_time)}-{seconds_to_hms(current_end)}]")
        elif debug:
            print(f"    üìä –°–æ–±—Ä–∞–Ω–æ: {len(texts)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤, {total_words} —Å–ª–æ–≤")

        # üÜï v16.14: –í–´–ë–ò–†–ê–ï–ú –î–û–ú–ò–ù–ò–†–£–Æ–©–ò–ô –°–ï–ì–ú–ï–ù–¢
        dominant_segment = max(all_segments_in_group, key=lambda s: len(s.get('text', '')))

        if len(all_segments_in_group) > 1:
            print(f"    üéØ –î–æ–º–∏–Ω–∏—Ä—É—é—â–∏–π: {dominant_segment.get('speaker')} / {dominant_segment.get('raw_speaker_id')} (–¥–ª–∏–Ω–∞: {len(dominant_segment.get('text', ''))} —Å–∏–º–≤–æ–ª–æ–≤)")

        # üÜï v17.4: –°–∫–ª–µ–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã —á–µ—Ä–µ–∑ join_texts_deduplicated (FIX –ë–ê–ì #17)
        if in_target_range or debug:
            print(f"    üîó –í—ã–∑–æ–≤ join_texts_deduplicated –¥–ª—è {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤...")

        final_text = join_texts_deduplicated(texts, debug=(in_target_range or debug))

        if in_target_range:
            print(f"\n     üìù –ü–æ—Å–ª–µ join_texts_deduplicated ({len(final_text)} —Å–∏–º–≤–æ–ª–æ–≤, {len(final_text.split())} —Å–ª–æ–≤):")
            print(f"        –ù–∞—á–∞–ª–æ: \"{final_text[:100]}...\"")
            print(f"        –ö–æ–Ω–µ—Ü:  \"...{final_text[-100:]}\"")

        if debug or in_target_range:
            print(f"    üßπ –í—ã–∑–æ–≤ clean_loops ({len(final_text)} —Å–∏–º–≤–æ–ª–æ–≤)...")

        final_text = clean_loops(final_text, debug=(debug or in_target_range))

        if in_target_range:
            print(f"\n     üìù –ü–æ—Å–ª–µ clean_loops ({len(final_text)} —Å–∏–º–≤–æ–ª–æ–≤, {len(final_text.split())} —Å–ª–æ–≤):")
            print(f"        –ù–∞—á–∞–ª–æ: \"{final_text[:100]}...\"")
            print(f"        –ö–æ–Ω–µ—Ü:  \"...{final_text[-100:]}\"")

        if debug or in_target_range:
            print(f"    üßπ –í—ã–∑–æ–≤ clean_hallucinations_from_text...")

        final_text = clean_hallucinations_from_text(final_text, current_speaker, debug=(debug or in_target_range))

        if in_target_range:
            print(f"\n     ‚úÖ –§–ò–ù–ê–õ–¨–ù–´–ô —Ç–µ–∫—Å—Ç ({len(final_text)} —Å–∏–º–≤–æ–ª–æ–≤, {len(final_text.split())} —Å–ª–æ–≤):")
            print(f"        –ù–∞—á–∞–ª–æ: \"{final_text[:100]}...\"")
            print(f"        –ö–æ–Ω–µ—Ü:  \"...{final_text[-100:]}\"")

            if "–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ" in final_text.lower():
                print(f"        ‚ùå –î–£–ë–õ–¨ ¬´–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ¬ª –í–°–Å –ï–©–Å –ï–°–¢–¨!")
            else:
                print(f"        ‚úÖ –î—É–±–ª—å ¬´–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ¬ª –£–°–¢–†–ê–ù–Å–ù!")
        elif debug:
            print(f"    ‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞, —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(final_text)} —Å–∏–º–≤–æ–ª–æ–≤")

        if final_text:
            # üÜï v17.10: sub_segments –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ timestamp injection (–í–∞—Ä–∏–∞–Ω—Ç A)
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö Whisper-—Å–µ–≥–º–µ–Ω—Ç–æ–≤ –î–û clean_loops
            merged.append({
                "speaker": dominant_segment.get('speaker', current_speaker),
                "time": current.get('start_hms', seconds_to_hms(start_time)),
                "start": start_time,
                "end": current_end,
                "text": final_text,
                "confidence": current.get('confidence', ''),
                "raw_speaker_id": dominant_segment.get('raw_speaker_id', ''),
                "sub_segments": [
                    {
                        "start": s["start"],
                        "end":   s["end"],
                        "words": len(s.get("text", "").split())
                    }
                    for s in all_segments_in_group
                ]
            })

            if len(texts) > 1:
                print(f"  ‚úÖ –°–∫–ª–µ–µ–Ω–æ {len(texts)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ ‚Üí {len(final_text.split())} —Å–ª–æ–≤")

        i = j

    if debug:
        print(f"\n‚úÖ merge_replicas –∑–∞–≤–µ—Ä—à—ë–Ω: {len(merged)} merged —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ {len(segments)} –∏—Å—Ö–æ–¥–Ω—ã—Ö")

    return merged
