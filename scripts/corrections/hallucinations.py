#!/usr/bin/env python3
"""
corrections/hallucinations.py - –£–¥–∞–ª–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π Whisper

üîß v16.27: –î–û–ë–ê–í–õ–ï–ù–ê clean_loops() - —É–¥–∞–ª–µ–Ω–∏–µ hallucination loops
- N-gram –∞–Ω–∞–ª–∏–∑ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ repeating patterns
- Adaptive threshold: –∫–æ—Ä–æ—Ç–∫–∏–µ (<5 —Å–ª–æ–≤) ‚Üí 85%, –¥–ª–∏–Ω–Ω—ã–µ ‚Üí 75%
- –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É n-–≥—Ä–∞–º–º–∞–º–∏

üÜï v16.19: –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô FIX - –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–µ–π + "–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–ª–µ–¥—É–µ—Ç"
- –î–µ—Ç–µ–∫—Ü–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑ (similarity >95%)
- –£–¥–∞–ª–µ–Ω–∏–µ Whisper hallucination –≤ –∫–æ–Ω—Ü–µ —Ñ–∞–π–ª–∞ ("–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–ª–µ–¥—É–µ—Ç", "–°–ø–∞—Å–∏–±–æ –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ")
- –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–µ–π —Å —Ä–∞–∑–Ω—ã–º —Ä–µ–≥–∏—Å—Ç—Ä–æ–º ("–ª–æ–≥–∏—á–Ω—ã–º –õ–æ–≥–∏—á–Ω—ã–º")
"""

import re
from difflib import SequenceMatcher


def is_hallucination(text):
    """
    ‚úÖ LEGACY FUNCTION (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å transcription.py)
    
    –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–µ–π Whisper.
    
    –¢–∏–ø–∏—á–Ω—ã–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏:
    - –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç
    - –¢–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—ã/–ø—É–Ω–∫—Ç—É–∞—Ü–∏—è
    - –ö–æ—Ä–æ—Ç–∫–∏–µ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã
    - –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã
    
    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    
    Returns:
        bool: True –µ—Å–ª–∏ —ç—Ç–æ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è
    """
    if not text or not text.strip():
        return True
    
    # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
    clean = re.sub(r'[^\w\s]', '', text)
    
    # –¢–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—ã
    if not clean.strip():
        return True
    
    # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (–º–µ–Ω—å—à–µ 3 —Å–∏–º–≤–æ–ª–æ–≤)
    if len(clean.strip()) < 3:
        return True
    
    # –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä "–∞–∞–∞–∞–∞")
    if len(set(clean.lower())) < 3:
        return True
    
    return False


def clean_loops(text, is_gap_filled=False, debug=False):
    """
    üÜï v16.27: –£–¥–∞–ª–µ–Ω–∏–µ hallucination loops - repeating n-grams
    
    Whisper –∏–Ω–æ–≥–¥–∞ —Å–æ–∑–¥–∞—ë—Ç –ø–µ—Ç–ª–∏ –ø—Ä–∏ force-transcribe gaps:
    "—É—á–∏—Ç—ã–≤–∞—Ç—å –±—ã–ª–∞ –Ω–µ–º–µ—Ü–∫–∞—è –∞—Ä—Ç–∏–ª–ª–µ—Ä–∏—è –≤–ø—Ä–∞–≤—å –¥–æ –µ—â–µ —Ñ–∞–∫—Ç–æ—Ä–æ–º
     –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–¥–æ —É—á–∏—Ç—ã–≤–∞—Ç—å —ç—Ç–æ –±—ã–ª–æ –Ω–µ–º–µ—Ü–∫–∞—è –≤–ø–ª–æ—Ç—å –¥–æ. –∫–æ—Ç–æ—Ä—ã–π
     –Ω–∞–¥–æ —É—á–∏—Ç—ã–≤–∞—Ç—å, —ç—Ç–æ –±—ã–ª–∞ –Ω–µ–º–µ—Ü–∫–∞—è –∞—Ä—Ç–∏–ª–ª–µ—Ä–∏—è, –≤–ø–ª–æ—Ç—å –¥–æ"
    
    –ê–ª–≥–æ—Ä–∏—Ç–º:
    1. –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞
    2. –°–æ–∑–¥–∞—ë—Ç n-–≥—Ä–∞–º–º—ã (2-8 —Å–ª–æ–≤)
    3. –ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ n-–≥—Ä–∞–º–º—ã (similarity > threshold)
    4. –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏, –æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
    
    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
        is_gap_filled: –≠—Ç–æ gap-filled —Å–µ–≥–º–µ–Ω—Ç (stricter threshold)
        debug: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å debug output
    
    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    if not text or len(text) < 20:
        return text
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º: –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è ‚Üí –ø—Ä–æ–±–µ–ª—ã
    normalized = re.sub(r'[^\w\s]', ' ', text.lower())
    words = [w for w in normalized.split() if w]
    
    if len(words) < 5:
        return text  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç
    
    # –ò—â–µ–º repeating n-grams
    removed_ranges = []  # [(start_word_idx, end_word_idx), ...]
    
    for ngram_size in range(8, 1, -1):  # –û—Ç –±–æ–ª—å—à–∏—Ö –∫ –º–∞–ª—ã–º (8‚Üí2)
        if ngram_size > len(words) // 2:
            continue
        
        # Adaptive threshold
        if ngram_size < 5:
            base_threshold = 0.85  # –ö–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã ‚Üí —Å—Ç—Ä–æ–∂–µ
        else:
            base_threshold = 0.75  # –î–ª–∏–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã ‚Üí –º—è–≥—á–µ
        
        # Gap-filled bonus
        threshold = base_threshold + (0.10 if is_gap_filled else 0.0)
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É n-–≥—Ä–∞–º–º–∞–º–∏
        min_distance = max(ngram_size // 2, 2)
        
        # –°–æ–∑–¥–∞—ë–º n-–≥—Ä–∞–º–º—ã
        ngrams = []
        for i in range(len(words) - ngram_size + 1):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ —É–¥–∞–ª—ë–Ω–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã
            if any(start <= i < end for start, end in removed_ranges):
                continue
            
            ngram = ' '.join(words[i:i+ngram_size])
            ngrams.append((i, ngram))
        
        # –ò—â–µ–º –ø–æ–≤—Ç–æ—Ä—ã
        for idx1, (pos1, ngram1) in enumerate(ngrams):
            if any(start <= pos1 < end for start, end in removed_ranges):
                continue
            
            for pos2, ngram2 in ngrams[idx1+1:]:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
                if pos2 - pos1 < min_distance:
                    continue
                
                # –£–∂–µ —É–¥–∞–ª–µ–Ω–æ?
                if any(start <= pos2 < end for start, end in removed_ranges):
                    continue
                
                # Similarity
                similarity = SequenceMatcher(None, ngram1, ngram2).ratio()
                
                if similarity >= threshold:
                    if debug:
                        print(f"  üîç LOOP (n={ngram_size}, sim={similarity:.0%}): \"{ngram1}\" ‚âà \"{ngram2}\"")
                    
                    # –£–¥–∞–ª—è–µ–º –≤—Ç–æ—Ä–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
                    removed_ranges.append((pos2, pos2 + ngram_size))
    
    # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ loops ‚Üí –ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç
    if removed_ranges:
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ start position
        removed_ranges.sort()
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏–µ—Å—è –¥–∏–∞–ø–∞–∑–æ–Ω—ã
        merged_ranges = []
        for start, end in removed_ranges:
            if merged_ranges and start < merged_ranges[-1][1]:
                # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ ‚Üí —Ä–∞—Å—à–∏—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω
                merged_ranges[-1] = (merged_ranges[-1][0], max(merged_ranges[-1][1], end))
            else:
                merged_ranges.append((start, end))
        
        # –°–æ–∑–¥–∞—ë–º –º–∞—Å–∫—É: –∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ –æ—Å—Ç–∞–≤–∏—Ç—å
        keep_mask = [True] * len(words)
        for start, end in merged_ranges:
            for i in range(start, min(end, len(words))):
                keep_mask[i] = False
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–æ–≤–∞
        original_words = text.split()  # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–≥–∏—Å—Ç—Ä
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ ‚Üí –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ (—Å –ø—É–Ω–∫—Ç—É–∞—Ü–∏–µ–π)
        # –ü—Ä–æ–±–ª–µ–º–∞: –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —É–±—Ä–∞–ª–∞ –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, –Ω—É–∂–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å
        # –†–µ—à–µ–Ω–∏–µ: –±–µ—Ä—ë–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞, –Ω–æ –ø—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É
        cleaned_words = []
        norm_idx = 0
        
        for orig_word in original_words:
            # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
            word_clean = re.sub(r'[^\w]', '', orig_word.lower())
            
            if word_clean:  # –ù–µ –ø—É—Å—Ç–æ–µ —Å–ª–æ–≤–æ
                if norm_idx < len(keep_mask) and keep_mask[norm_idx]:
                    cleaned_words.append(orig_word)
                norm_idx += 1
            else:
                # –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä "—Å–ª–æ–≤–æ. —Å–ª–æ–≤–æ")
                # –î–æ–±–∞–≤–ª—è–µ–º –µ—Å–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Å–ª–æ–≤–æ –Ω–µ —É–¥–∞–ª–µ–Ω–æ
                if cleaned_words:
                    cleaned_words.append(orig_word)
        
        cleaned_text = ' '.join(cleaned_words)
        
        # –û—á–∏—Å—Ç–∫–∞ –¥–≤–æ–π–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
        
        if debug:
            print(f"  ‚úÖ Loops removed: {len(original_words)} ‚Üí {len(cleaned_words)} —Å–ª–æ–≤")
        
        return cleaned_text
    
    return text


def is_duplicate_phrase(text, debug=False):
    """
    üÜï v16.19: –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã
    
    –ü—Ä–∏–º–µ—Ä—ã –¥—É–±–ª–µ–π:
    - "–Ω–∏—á–µ–≥–æ –Ω–µ –∑–Ω–∞–ª–∏. –Ω–∏—á–µ–≥–æ –Ω–µ –∑–Ω–∞–ª–∏."
    - "–ª–æ–≥–∏—á–Ω—ã–º –õ–æ–≥–∏—á–Ω—ã–º —Ä–µ—à–µ–Ω–∏–µ–º"
    - "–Ω–∞—á–∏–Ω–∞–µ—Ç –Ω–∞—Å—Ç—É–ø–∞—Ç—å –Ω–∞—á–∏–Ω–∞–µ—Ç –Ω–∞—Å—Ç—É–ø–∞—Ç—å"
    
    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        debug: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å debug output
    
    Returns:
        (has_duplicate, cleaned_text)
    """
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    sentences = re.split(r'([.!?]+)\s*', text)
    sentences = [s.strip() for s in sentences if s.strip() and s not in '.!?']
    
    if len(sentences) < 2:
        return False, text
    
    # –ò—â–µ–º —Å–º–µ–∂–Ω—ã–µ –¥—É–±–ª–∏
    cleaned_sentences = []
    skip_next = False
    duplicates_found = 0
    
    for i in range(len(sentences)):
        if skip_next:
            skip_next = False
            continue
        
        current = sentences[i]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ª–µ–¥—É—é—â–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        if i < len(sentences) - 1:
            next_sent = sentences[i + 1]
            
            # Similarity (–∏–≥–Ω–æ—Ä–∏—Ä—É—è —Ä–µ–≥–∏—Å—Ç—Ä)
            similarity = SequenceMatcher(
                None, 
                current.lower().strip(), 
                next_sent.lower().strip()
            ).ratio()
            
            if similarity > 0.95:  # 95% similarity = –¥—É–±–ª—å!
                if debug:
                    print(f"  üîç –î–£–ë–õ–¨ (similarity={similarity:.2%}): \"{current}\" ‚âà \"{next_sent}\"")
                
                # –ë–µ—Ä—ë–º –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
                if len(next_sent) > len(current):
                    cleaned_sentences.append(next_sent)
                else:
                    cleaned_sentences.append(current)
                
                skip_next = True
                duplicates_found += 1
                continue
        
        cleaned_sentences.append(current)
    
    if duplicates_found > 0:
        cleaned_text = '. '.join(cleaned_sentences) + '.'
        return True, cleaned_text
    
    return False, text


def remove_ending_hallucinations(text, debug=False):
    """
    üÜï v16.19: –£–¥–∞–ª—è–µ—Ç —Ç–∏–ø–∏—á–Ω—ã–µ Whisper hallucination –≤ –∫–æ–Ω—Ü–µ —Ç–µ–∫—Å—Ç–∞
    
    Whisper —á–∞—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ—Ç –≤ –∫–æ–Ω—Ü–µ —Ñ–∞–π–ª–∞:
    - "–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–ª–µ–¥—É–µ—Ç"
    - "–°–ø–∞—Å–∏–±–æ –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ"
    - "–î–æ –Ω–æ–≤—ã—Ö –≤—Å—Ç—Ä–µ—á"
    - "–ü–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å –Ω–∞ –Ω–∞—à –∫–∞–Ω–∞–ª"
    
    Args:
        text: –¢–µ–∫—Å—Ç —Å–µ–≥–º–µ–Ω—Ç–∞
        debug: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å debug output
    
    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    hallucination_patterns = [
        r'–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ\s+—Å–ª–µ–¥—É–µ—Ç[.!?]*\s*$',
        r'—Å–ø–∞—Å–∏–±–æ\s+–∑–∞\s+–≤–Ω–∏–º–∞–Ω–∏–µ[.!?]*\s*$',
        r'–¥–æ\s+–Ω–æ–≤—ã—Ö\s+–≤—Å—Ç—Ä–µ—á[.!?]*\s*$',
        r'–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å\s+–Ω–∞\s+–Ω–∞—à\s+–∫–∞–Ω–∞–ª[.!?]*\s*$',
        r'—Å—Ç–∞–≤—å—Ç–µ\s+–ª–∞–π–∫–∏[.!?]*\s*$',
    ]
    
    text_lower = text.lower()
    
    for pattern in hallucination_patterns:
        if re.search(pattern, text_lower):
            cleaned = re.sub(pattern, '', text, flags=re.IGNORECASE).strip()
            
            if debug:
                removed = text[len(cleaned):].strip()
                print(f"  üóëÔ∏è HALLUCINATION: —É–¥–∞–ª–µ–Ω–∞ —Ñ—Ä–∞–∑–∞ \"{removed}\"")
            
            return cleaned
    
    return text


def clean_hallucinations_from_text(text, speaker=None, is_gap_filled=False, debug=False):
    """
    üîß v16.27: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
    
    –í—ã–ø–æ–ª–Ω—è–µ—Ç:
    1. üÜï –£–¥–∞–ª–µ–Ω–∏–µ hallucination loops (clean_loops)
    2. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑
    3. –£–¥–∞–ª–µ–Ω–∏–µ ending hallucinations
    4. –û—á–∏—Å—Ç–∫–∞ multiple –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    
    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
        speaker: –°–ø–∏–∫–µ—Ä (–¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
        is_gap_filled: –≠—Ç–æ gap-filled —Å–µ–≥–º–µ–Ω—Ç (stricter threshold)
        debug: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å debug output
    
    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    if not text or not text.strip():
        return text
    
    original_text = text
    
    # 1. üÜï v16.27: –£–¥–∞–ª–µ–Ω–∏–µ hallucination loops
    text = clean_loops(text, is_gap_filled=is_gap_filled, debug=debug)
    
    # 2. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–µ–π
    has_dupl, text = is_duplicate_phrase(text, debug=debug)
    
    # 3. –£–¥–∞–ª–µ–Ω–∏–µ ending hallucinations
    text = remove_ending_hallucinations(text, debug=debug)
    
    # 4. –û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    text = re.sub(r'\s+', ' ', text)  # Multiple spaces ‚Üí one
    text = re.sub(r'([.!?]){2,}', r'\1', text)  # Multiple punctuation ‚Üí one
    text = text.strip()
    
    if debug and text != original_text:
        print(f"  ‚úÖ –û—á–∏—â–µ–Ω–æ: {len(original_text)} ‚Üí {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    return text


def filter_hallucination_segments(segments, debug=True):
    """
    üîß v16.27: –§–∏–ª—å—Ç—Ä—É–µ—Ç —Å–µ–≥–º–µ–Ω—Ç—ã –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
    
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç clean_hallucinations_from_text() –∫ –∫–∞–∂–¥–æ–º—É —Å–µ–≥–º–µ–Ω—Ç—É.
    –£–¥–∞–ª—è–µ—Ç —Å–µ–≥–º–µ–Ω—Ç—ã, —Å—Ç–∞–≤—à–∏–µ –ø—É—Å—Ç—ã–º–∏ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏.
    –ü–µ—Ä–µ–¥–∞—ë—Ç is_gap_filled —Ñ–ª–∞–≥ –¥–ª—è gap-filled —Å–µ–≥–º–µ–Ω—Ç–æ–≤.
    
    Args:
        segments: –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        debug: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å debug output
    
    Returns:
        –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
    """
    if debug:
        print(f"\nüßπ –û—á–∏—Å—Ç–∫–∞ hallucinations –∏–∑ {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤...")
    
    cleaned_segments = []
    removed_count = 0
    
    for seg in segments:
        text = seg.get('text', '')
        speaker = seg.get('speaker', '')
        is_gap_filled = seg.get('source') == 'GAP_FILLED'  # üÜï v16.27
        
        cleaned_text = clean_hallucinations_from_text(
            text, speaker, is_gap_filled=is_gap_filled, debug=debug
        )
        
        if cleaned_text:
            seg['text'] = cleaned_text
            cleaned_segments.append(seg)
        else:
            removed_count += 1
            if debug:
                print(f"  üóëÔ∏è –£–¥–∞–ª—ë–Ω –ø—É—Å—Ç–æ–π —Å–µ–≥–º–µ–Ω—Ç: {seg.get('time', '???')} ({speaker})")
    
    if debug:
        print(f"‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(cleaned_segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (—É–¥–∞–ª–µ–Ω–æ {removed_count})")
    
    return cleaned_segments
