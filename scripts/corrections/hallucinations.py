#!/usr/bin/env python3
"""
corrections/hallucinations.py - –£–¥–∞–ª–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π Whisper v16.19

üÜï v16.19: –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô FIX - –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–µ–π + "–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–ª–µ–¥—É–µ—Ç"
- –î–µ—Ç–µ–∫—Ü–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑ (similarity >95%)
- –£–¥–∞–ª–µ–Ω–∏–µ Whisper hallucination –≤ –∫–æ–Ω—Ü–µ —Ñ–∞–π–ª–∞ ("–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–ª–µ–¥—É–µ—Ç", "–°–ø–∞—Å–∏–±–æ –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ")
- –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–µ–π —Å —Ä–∞–∑–Ω—ã–º —Ä–µ–≥–∏—Å—Ç—Ä–æ–º ("–ª–æ–≥–∏—á–Ω—ã–º –õ–æ–≥–∏—á–Ω—ã–º")
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è is_hallucination() –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
"""

import re
from difflib import SequenceMatcher


def is_hallucination(text):
    """
    ‚úÖ LEGACY FUNCTION (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å transcription.py)
    
    –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–µ–π Whisper.
    
    –¢–∏–ø–∏—á–Ω—ã–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏:
    - –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç
    - –¢–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—ã/–ø—É–Ω–∫—Ç—É–∞—Ü–∏—è
    - –ö–æ—Ä–æ—Ç–∫–∏–µ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã
    - –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã
    
    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    
    Returns:
        bool: True –µ—Å–ª–∏ —ç—Ç–æ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è
    """
    if not text or not text.strip():
        return True
    
    # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
    clean = re.sub(r'[^\w\s]', '', text)
    
    # –¢–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—ã
    if not clean.strip():
        return True
    
    # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (–º–µ–Ω—å—à–µ 3 —Å–∏–º–≤–æ–ª–æ–≤)
    if len(clean.strip()) < 3:
        return True
    
    # –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä "–∞–∞–∞–∞–∞")
    if len(set(clean.lower())) < 3:
        return True
    
    return False


def is_duplicate_phrase(text, debug=False):
    """
    üÜï v16.19: –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã
    
    –ü—Ä–∏–º–µ—Ä—ã –¥—É–±–ª–µ–π:
    - "–Ω–∏—á–µ–≥–æ –Ω–µ –∑–Ω–∞–ª–∏. –Ω–∏—á–µ–≥–æ –Ω–µ –∑–Ω–∞–ª–∏."
    - "–ª–æ–≥–∏—á–Ω—ã–º –õ–æ–≥–∏—á–Ω—ã–º —Ä–µ—à–µ–Ω–∏–µ–º"
    - "–Ω–∞—á–∏–Ω–∞–µ—Ç –Ω–∞—Å—Ç—É–ø–∞—Ç—å –Ω–∞—á–∏–Ω–∞–µ—Ç –Ω–∞—Å—Ç—É–ø–∞—Ç—å"
    
    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        debug: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å debug output
    
    Returns:
        (has_duplicate, cleaned_text)
    """
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    sentences = re.split(r'([.!?]+)\s*', text)
    sentences = [s.strip() for s in sentences if s.strip() and s not in '.!?']
    
    if len(sentences) < 2:
        return False, text
    
    # –ò—â–µ–º —Å–º–µ–∂–Ω—ã–µ –¥—É–±–ª–∏
    cleaned_sentences = []
    skip_next = False
    duplicates_found = 0
    
    for i in range(len(sentences)):
        if skip_next:
            skip_next = False
            continue
        
        current = sentences[i]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ª–µ–¥—É—é—â–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        if i < len(sentences) - 1:
            next_sent = sentences[i + 1]
            
            # Similarity (–∏–≥–Ω–æ—Ä–∏—Ä—É—è —Ä–µ–≥–∏—Å—Ç—Ä)
            similarity = SequenceMatcher(
                None, 
                current.lower().strip(), 
                next_sent.lower().strip()
            ).ratio()
            
            if similarity > 0.95:  # 95% similarity = –¥—É–±–ª—å!
                if debug:
                    print(f"  üîç –î–£–ë–õ–¨ (similarity={similarity:.2%}): \"{current}\" ‚âà \"{next_sent}\"")
                
                # –ë–µ—Ä—ë–º –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
                if len(next_sent) > len(current):
                    cleaned_sentences.append(next_sent)
                else:
                    cleaned_sentences.append(current)
                
                skip_next = True
                duplicates_found += 1
                continue
        
        cleaned_sentences.append(current)
    
    if duplicates_found > 0:
        cleaned_text = '. '.join(cleaned_sentences) + '.'
        return True, cleaned_text
    
    return False, text


def remove_ending_hallucinations(text, debug=False):
    """
    üÜï v16.19: –£–¥–∞–ª—è–µ—Ç —Ç–∏–ø–∏—á–Ω—ã–µ Whisper hallucination –≤ –∫–æ–Ω—Ü–µ —Ç–µ–∫—Å—Ç–∞
    
    Whisper —á–∞—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ—Ç –≤ –∫–æ–Ω—Ü–µ —Ñ–∞–π–ª–∞:
    - "–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–ª–µ–¥—É–µ—Ç"
    - "–°–ø–∞—Å–∏–±–æ –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ"
    - "–î–æ –Ω–æ–≤—ã—Ö –≤—Å—Ç—Ä–µ—á"
    - "–ü–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å –Ω–∞ –Ω–∞—à –∫–∞–Ω–∞–ª"
    
    Args:
        text: –¢–µ–∫—Å—Ç —Å–µ–≥–º–µ–Ω—Ç–∞
        debug: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å debug output
    
    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    hallucination_patterns = [
        r'–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ\s+—Å–ª–µ–¥—É–µ—Ç[.!?]*\s*$',
        r'—Å–ø–∞—Å–∏–±–æ\s+–∑–∞\s+–≤–Ω–∏–º–∞–Ω–∏–µ[.!?]*\s*$',
        r'–¥–æ\s+–Ω–æ–≤—ã—Ö\s+–≤—Å—Ç—Ä–µ—á[.!?]*\s*$',
        r'–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å\s+–Ω–∞\s+–Ω–∞—à\s+–∫–∞–Ω–∞–ª[.!?]*\s*$',
        r'—Å—Ç–∞–≤—å—Ç–µ\s+–ª–∞–π–∫–∏[.!?]*\s*$',
    ]
    
    text_lower = text.lower()
    
    for pattern in hallucination_patterns:
        if re.search(pattern, text_lower):
            cleaned = re.sub(pattern, '', text, flags=re.IGNORECASE).strip()
            
            if debug:
                removed = text[len(cleaned):].strip()
                print(f"  üóëÔ∏è HALLUCINATION: —É–¥–∞–ª–µ–Ω–∞ —Ñ—Ä–∞–∑–∞ \"{removed}\"")
            
            return cleaned
    
    return text


def clean_hallucinations_from_text(text, speaker=None, debug=False):
    """
    üÜï v16.19: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
    
    –í—ã–ø–æ–ª–Ω—è–µ—Ç:
    1. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑
    2. –£–¥–∞–ª–µ–Ω–∏–µ ending hallucinations
    3. –û—á–∏—Å—Ç–∫–∞ multiple –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    
    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
        speaker: –°–ø–∏–∫–µ—Ä (–¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
        debug: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å debug output
    
    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    if not text or not text.strip():
        return text
    
    original_text = text
    
    # 1. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–µ–π
    has_dupl, text = is_duplicate_phrase(text, debug=debug)
    
    # 2. –£–¥–∞–ª–µ–Ω–∏–µ ending hallucinations
    text = remove_ending_hallucinations(text, debug=debug)
    
    # 3. –û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    text = re.sub(r'\s+', ' ', text)  # Multiple spaces ‚Üí one
    text = re.sub(r'([.!?]){2,}', r'\1', text)  # Multiple punctuation ‚Üí one
    text = text.strip()
    
    if debug and text != original_text:
        print(f"  ‚úÖ –û—á–∏—â–µ–Ω–æ: {len(original_text)} ‚Üí {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    return text


def filter_hallucination_segments(segments, debug=True):
    """
    üÜï v16.19: –§–∏–ª—å—Ç—Ä—É–µ—Ç —Å–µ–≥–º–µ–Ω—Ç—ã –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
    
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç clean_hallucinations_from_text() –∫ –∫–∞–∂–¥–æ–º—É —Å–µ–≥–º–µ–Ω—Ç—É.
    –£–¥–∞–ª—è–µ—Ç —Å–µ–≥–º–µ–Ω—Ç—ã, —Å—Ç–∞–≤—à–∏–µ –ø—É—Å—Ç—ã–º–∏ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏.
    
    Args:
        segments: –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        debug: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å debug output
    
    Returns:
        –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
    """
    if debug:
        print(f"\nüßπ –û—á–∏—Å—Ç–∫–∞ hallucinations –∏–∑ {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤...")
    
    cleaned_segments = []
    removed_count = 0
    
    for seg in segments:
        text = seg.get('text', '')
        speaker = seg.get('speaker', '')
        
        cleaned_text = clean_hallucinations_from_text(text, speaker, debug=debug)
        
        if cleaned_text:
            seg['text'] = cleaned_text
            cleaned_segments.append(seg)
        else:
            removed_count += 1
            if debug:
                print(f"  üóëÔ∏è –£–¥–∞–ª—ë–Ω –ø—É—Å—Ç–æ–π —Å–µ–≥–º–µ–Ω—Ç: {seg.get('time', '???')} ({speaker})")
    
    if debug:
        print(f"‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(cleaned_segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (—É–¥–∞–ª–µ–Ω–æ {removed_count})")
    
    return cleaned_segments

def mark_low_confidence_words(text: str, words: list, prob_threshold: float = 0.35, min_len: int = 4) -> str:
    """
    –ó–∞–º–µ–Ω—è–µ—Ç –Ω–∏–∑–∫–æ—É–≤–µ—Ä–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ Whisper –Ω–∞ [–Ω—Ä–∑–±].

    –û–∂–∏–¥–∞–µ—Ç—Å—è words –≤–∏–¥–∞: [{'word': '...', 'probability': 0.xx, ...}, ...]
    """
    if not text or not words:
        return text

    out = []
    prev_nrzb = False

    for w in words:
        wtxt = (w.get("word") or "").strip()
        prob = w.get("probability", None)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º "–¥–ª–∏–Ω—É —Å–ª–æ–≤–∞" –±–µ–∑ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        core = re.sub(r"[^\w–∞-—è–ê-–Ø—ë–Å]", "", wtxt)

        low_conf = (prob is not None and prob < prob_threshold and len(core) >= min_len)

        if low_conf:
            if not prev_nrzb:
                out.append("[–Ω—Ä–∑–±]")
                prev_nrzb = True
            # –µ—Å–ª–∏ –ø–æ–¥—Ä—è–¥ –Ω–µ—Å–∫–æ–ª—å–∫–æ low_conf ‚Äî —Å—Ö–ª–æ–ø—ã–≤–∞–µ–º –≤ –æ–¥–∏–Ω [–Ω—Ä–∑–±]
        else:
            if wtxt:
                out.append(wtxt)
                prev_nrzb = False

    return " ".join(out).strip()
