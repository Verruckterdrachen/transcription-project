#!/usr/bin/env python3
"""
corrections/hallucinations.py - –£–¥–∞–ª–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π Whisper v16.23

üÜï v16.23: FIX –ë–ê–ì #3 - Hallucination loops –≤ gap-filled —Å–µ–≥–º–µ–Ω—Ç–∞—Ö
- –ù–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è clean_loops() –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ repeating patterns
- Adaptive threshold: –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã (<5 —Å–ª–æ–≤) ‚Üí 85%, –¥–ª–∏–Ω–Ω—ã–µ ‚Üí 75%
- –û—Å–æ–±–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ gap-filled —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (threshold +10%)

üÜï v16.19: –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô FIX - –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–µ–π + "–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–ª–µ–¥—É–µ—Ç"
- –î–µ—Ç–µ–∫—Ü–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑ (similarity >95%)
- –£–¥–∞–ª–µ–Ω–∏–µ Whisper hallucination –≤ –∫–æ–Ω—Ü–µ —Ñ–∞–π–ª–∞ ("–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–ª–µ–¥—É–µ—Ç", "–°–ø–∞—Å–∏–±–æ –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ")
- –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–µ–π —Å —Ä–∞–∑–Ω—ã–º —Ä–µ–≥–∏—Å—Ç—Ä–æ–º ("–ª–æ–≥–∏—á–Ω—ã–º –õ–æ–≥–∏—á–Ω—ã–º")
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è is_hallucination() –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
"""

import re
from difflib import SequenceMatcher


def is_hallucination(text):
    """
    ‚úÖ LEGACY FUNCTION (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å transcription.py)
    
    –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–µ–π Whisper.
    
    –¢–∏–ø–∏—á–Ω—ã–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏:
    - –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç
    - –¢–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—ã/–ø—É–Ω–∫—Ç—É–∞—Ü–∏—è
    - –ö–æ—Ä–æ—Ç–∫–∏–µ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã
    - –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã
    
    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    
    Returns:
        bool: True –µ—Å–ª–∏ —ç—Ç–æ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è
    """
    if not text or not text.strip():
        return True
    
    # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
    clean = re.sub(r'[^\w\s]', '', text)
    
    # –¢–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—ã
    if not clean.strip():
        return True
    
    # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (–º–µ–Ω—å—à–µ 3 —Å–∏–º–≤–æ–ª–æ–≤)
    if len(clean.strip()) < 3:
        return True
    
    # –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä "–∞–∞–∞–∞–∞")
    if len(set(clean.lower())) < 3:
        return True
    
    return False


def is_duplicate_phrase(text, debug=False):
    """
    üÜï v16.19: –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã
    
    –ü—Ä–∏–º–µ—Ä—ã –¥—É–±–ª–µ–π:
    - "–Ω–∏—á–µ–≥–æ –Ω–µ –∑–Ω–∞–ª–∏. –Ω–∏—á–µ–≥–æ –Ω–µ –∑–Ω–∞–ª–∏."
    - "–ª–æ–≥–∏—á–Ω—ã–º –õ–æ–≥–∏—á–Ω—ã–º —Ä–µ—à–µ–Ω–∏–µ–º"
    - "–Ω–∞—á–∏–Ω–∞–µ—Ç –Ω–∞—Å—Ç—É–ø–∞—Ç—å –Ω–∞—á–∏–Ω–∞–µ—Ç –Ω–∞—Å—Ç—É–ø–∞—Ç—å"
    
    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        debug: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å debug output
    
    Returns:
        (has_duplicate, cleaned_text)
    """
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    sentences = re.split(r'([.!?]+)\s*', text)
    sentences = [s.strip() for s in sentences if s.strip() and s not in '.!?']
    
    if len(sentences) < 2:
        return False, text
    
    # –ò—â–µ–º —Å–º–µ–∂–Ω—ã–µ –¥—É–±–ª–∏
    cleaned_sentences = []
    skip_next = False
    duplicates_found = 0
    
    for i in range(len(sentences)):
        if skip_next:
            skip_next = False
            continue
        
        current = sentences[i]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ª–µ–¥—É—é—â–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        if i < len(sentences) - 1:
            next_sent = sentences[i + 1]
            
            # Similarity (–∏–≥–Ω–æ—Ä–∏—Ä—É—è —Ä–µ–≥–∏—Å—Ç—Ä)
            similarity = SequenceMatcher(
                None, 
                current.lower().strip(), 
                next_sent.lower().strip()
            ).ratio()
            
            if similarity > 0.95:  # 95% similarity = –¥—É–±–ª—å!
                if debug:
                    print(f"  üîç –î–£–ë–õ–¨ (similarity={similarity:.2%}): \"{current}\" ‚âà \"{next_sent}\"")
                
                # –ë–µ—Ä—ë–º –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
                if len(next_sent) > len(current):
                    cleaned_sentences.append(next_sent)
                else:
                    cleaned_sentences.append(current)
                
                skip_next = True
                duplicates_found += 1
                continue
        
        cleaned_sentences.append(current)
    
    if duplicates_found > 0:
        cleaned_text = '. '.join(cleaned_sentences) + '.'
        return True, cleaned_text
    
    return False, text

def clean_loops(text, is_gap_filled=False, debug=False):
    """
    üÜï v16.23: –£–¥–∞–ª—è–µ—Ç repeating loop patterns –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—Å—Ç–∞
    
    –î–µ—Ç–µ–∫—Ç–∏—Ç –∏ —É–¥–∞–ª—è–µ—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ñ—Ä–∞–∑—ã —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏:
    - "—É—á–∏—Ç—ã–≤–∞—Ç—å –±—ã–ª–∞ –Ω–µ–º–µ—Ü–∫–∞—è... –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–¥–æ —É—á–∏—Ç—ã–≤–∞—Ç—å... —ç—Ç–æ –±—ã–ª–∞ –Ω–µ–º–µ—Ü–∫–∞—è"
    - "–≤–ø–ª–æ—Ç—å –¥–æ... –≤–ø—Ä–∞–≤—å –¥–æ... –≤–ø–ª–æ—Ç—å –¥–æ"
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç n-gram –∞–Ω–∞–ª–∏–∑ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —Ñ—Ä–∞–∑.
    
    Adaptive threshold:
    - –ö–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã (<5 —Å–ª–æ–≤): similarity ‚â•85%
    - –î–ª–∏–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã (‚â•5 —Å–ª–æ–≤): similarity ‚â•75%
    - Gap-filled —Å–µ–≥–º–µ–Ω—Ç—ã: +10% –∫ threshold
    
    –í–ê–ñ–ù–û: n-–≥—Ä–∞–º–º—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ–º,
    —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å false positives –æ—Ç overlapping windows.
    
    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
        is_gap_filled: True –µ—Å–ª–∏ —Å–µ–≥–º–µ–Ω—Ç –±—ã–ª –¥–æ–±–∞–≤–ª–µ–Ω —á–µ—Ä–µ–∑ gap filling
        debug: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å debug output
    
    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    if not text or len(text.strip()) < 20:
        return text
    
    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞)
    words = re.findall(r'\b\w+\b', text.lower())
    
    if len(words) < 4:  # ‚Üê –ò–ó–ú–ï–ù–ï–ù–û: –±—ã–ª–æ 6, —Å—Ç–∞–ª–æ 4 (–º–∏–Ω–∏–º—É–º 2+2 –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏)
        return text  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    min_ngram_size = 2  # ‚Üê –ò–ó–ú–ï–ù–ï–ù–û: –±—ã–ª–æ 3, —Å—Ç–∞–ª–æ 2 (–¥–µ—Ç–µ–∫—Ç–∏–º —Ñ—Ä–∞–∑—ã –∏–∑ 2 —Å–ª–æ–≤!)
    max_ngram_size = 8  # –ú–∞–∫—Å–∏–º—É–º 8 —Å–ª–æ–≤
    
    removed_positions = set()  # –ü–æ–∑–∏—Ü–∏–∏ —Å–ª–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
    loop_found = False
    
    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã n-gram (–æ—Ç –±–æ–ª—å—à–µ–≥–æ –∫ –º–µ–Ω—å—à–µ–º—É)
    for ngram_size in range(max_ngram_size, min_ngram_size - 1, -1):
        
        # Adaptive threshold
        if ngram_size < 5:
            base_threshold = 0.85  # –ö–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã ‚Äî —Å—Ç—Ä–æ–∂–µ!
        else:
            base_threshold = 0.75
        
        # Gap-filled —Å–µ–≥–º–µ–Ω—Ç—ã —Ç—Ä–µ–±—É—é—Ç –µ—â—ë –±–æ–ª—å—à–µ –≤–Ω–∏–º–∞–Ω–∏—è
        threshold = base_threshold + (0.10 if is_gap_filled else 0.0)
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É n-–≥—Ä–∞–º–º–∞–º–∏ (—á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å overlapping)
        min_distance = max(ngram_size // 2, 2)  # –ú–∏–Ω–∏–º—É–º –ø–æ–ª–æ–≤–∏–Ω–∞ —Ä–∞–∑–º–µ—Ä–∞ n-–≥—Ä–∞–º–º—ã –∏–ª–∏ 2 —Å–ª–æ–≤–∞
        
        # –°–æ–∑–¥–∞—ë–º –≤—Å–µ n-–≥—Ä–∞–º–º—ã –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        ngrams = []
        for i in range(len(words) - ngram_size + 1):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º n-–≥—Ä–∞–º–º—ã —Å —É–¥–∞–ª—ë–Ω–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
            if any(idx in removed_positions for idx in range(i, i + ngram_size)):
                continue
            
            ngram = ' '.join(words[i:i + ngram_size])
            ngrams.append((i, ngram))
        
        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ n-–≥—Ä–∞–º–º—ã
        matched_pairs = []
        
        for i, (pos1, ngram1) in enumerate(ngrams):
            for pos2, ngram2 in ngrams[i + 1:]:
                
                # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: n-–≥—Ä–∞–º–º—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–ª–µ–∫–æ –¥—Ä—É–≥ –æ—Ç –¥—Ä—É–≥–∞!
                distance = pos2 - pos1
                if distance < min_distance:
                    continue  # –°–ª–∏—à–∫–æ–º –±–ª–∏–∑–∫–æ ‚Äî —ç—Ç–æ overlapping, –Ω–µ loop!
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º similarity
                similarity = SequenceMatcher(None, ngram1, ngram2).ratio()
                
                if similarity >= threshold:
                    matched_pairs.append((pos1, pos2, ngram1, ngram2, similarity))
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–∞—Ä—ã
        for pos1, pos2, ngram1, ngram2, similarity in matched_pairs:
            # LOOP DETECTED!
            if debug:
                print(f"  üîÑ LOOP (len={ngram_size}, sim={similarity:.0%}, distance={pos2-pos1}):")
                print(f"     [{pos1}] \"{ngram1}\"")
                print(f"     [{pos2}] \"{ngram2}\"")
            
            # –£–¥–∞–ª—è–µ–º –≤—Ç–æ—Ä—É—é –∫–æ–ø–∏—é (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—É—é)
            for idx in range(pos2, pos2 + ngram_size):
                removed_positions.add(idx)
            
            loop_found = True
    
    if not loop_found:
        return text
    
    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –±–µ–∑ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö —Å–ª–æ–≤
    words_original = re.findall(r'\b\w+\b', text)
    
    # –°–æ–∑–¥–∞—ë–º –º–∞–ø–ø–∏–Ω–≥: –ø–æ–∑–∏—Ü–∏—è —Å–ª–æ–≤–∞ ‚Üí –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ
    kept_words = [words_original[i] for i in range(len(words_original)) if i not in removed_positions]
    
    if not kept_words:
        return text  # –ï—Å–ª–∏ –≤—Å—ë —É–¥–∞–ª–µ–Ω–æ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª
    
    # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Å–ª–æ–≤
    result_parts = []
    word_idx = 0
    
    for match in re.finditer(r'\b\w+\b|[^\w\s]', text):
        token = match.group()
        
        if re.match(r'\w+', token):  # –≠—Ç–æ —Å–ª–æ–≤–æ
            if word_idx not in removed_positions:
                result_parts.append(token)
            word_idx += 1
        else:  # –≠—Ç–æ –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Å–ª–æ–≤–æ –Ω–µ —É–¥–∞–ª–µ–Ω–æ
            if result_parts:
                result_parts.append(token)
    
    cleaned_text = ' '.join(result_parts)
    
    # –û—á–∏—Å—Ç–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
    cleaned_text = re.sub(r'\s+([.,;:!?])', r'\1', cleaned_text)
    cleaned_text = re.sub(r'([.,;:!?])\s*\1+', r'\1', cleaned_text)  # "... ..." ‚Üí "..."
    
    if debug:
        print(f"  ‚úÖ LOOP —É–¥–∞–ª—ë–Ω: {len(text)} ‚Üí {len(cleaned_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"     –ë–´–õ–û: {text[:80]}...")
        print(f"     –°–¢–ê–õ–û: {cleaned_text[:80]}...")
    
    return cleaned_text.strip()

def remove_ending_hallucinations(text, debug=False):
    """
    üÜï v16.19: –£–¥–∞–ª—è–µ—Ç —Ç–∏–ø–∏—á–Ω—ã–µ Whisper hallucination –≤ –∫–æ–Ω—Ü–µ —Ç–µ–∫—Å—Ç–∞
    
    Whisper —á–∞—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ—Ç –≤ –∫–æ–Ω—Ü–µ —Ñ–∞–π–ª–∞:
    - "–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–ª–µ–¥—É–µ—Ç"
    - "–°–ø–∞—Å–∏–±–æ –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ"
    - "–î–æ –Ω–æ–≤—ã—Ö –≤—Å—Ç—Ä–µ—á"
    - "–ü–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å –Ω–∞ –Ω–∞—à –∫–∞–Ω–∞–ª"
    
    Args:
        text: –¢–µ–∫—Å—Ç —Å–µ–≥–º–µ–Ω—Ç–∞
        debug: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å debug output
    
    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    hallucination_patterns = [
        r'–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ\s+—Å–ª–µ–¥—É–µ—Ç[.!?]*\s*$',
        r'—Å–ø–∞—Å–∏–±–æ\s+–∑–∞\s+–≤–Ω–∏–º–∞–Ω–∏–µ[.!?]*\s*$',
        r'–¥–æ\s+–Ω–æ–≤—ã—Ö\s+–≤—Å—Ç—Ä–µ—á[.!?]*\s*$',
        r'–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å\s+–Ω–∞\s+–Ω–∞—à\s+–∫–∞–Ω–∞–ª[.!?]*\s*$',
        r'—Å—Ç–∞–≤—å—Ç–µ\s+–ª–∞–π–∫–∏[.!?]*\s*$',
    ]
    
    text_lower = text.lower()
    
    for pattern in hallucination_patterns:
        if re.search(pattern, text_lower):
            cleaned = re.sub(pattern, '', text, flags=re.IGNORECASE).strip()
            
            if debug:
                removed = text[len(cleaned):].strip()
                print(f"  üóëÔ∏è HALLUCINATION: —É–¥–∞–ª–µ–Ω–∞ —Ñ—Ä–∞–∑–∞ \"{removed}\"")
            
            return cleaned
    
    return text


def clean_hallucinations_from_text(text, speaker=None, is_gap_filled=False, debug=False):
    """
    üÜï v16.23: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
    
    –í—ã–ø–æ–ª–Ω—è–µ—Ç:
    1. –£–¥–∞–ª–µ–Ω–∏–µ loop patterns (–Ω–æ–≤–æ–µ!)
    2. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑
    3. –£–¥–∞–ª–µ–Ω–∏–µ ending hallucinations
    4. –û—á–∏—Å—Ç–∫–∞ multiple –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    
    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
        speaker: –°–ø–∏–∫–µ—Ä (–¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
        is_gap_filled: True –µ—Å–ª–∏ —Å–µ–≥–º–µ–Ω—Ç –±—ã–ª –¥–æ–±–∞–≤–ª–µ–Ω —á–µ—Ä–µ–∑ gap filling
        debug: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å debug output
    
    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    if not text or not text.strip():
        return text
    
    original_text = text
    
    # 1. –£–¥–∞–ª–µ–Ω–∏–µ loop patterns (–ë–ê–ì #3 FIX!)
    text = clean_loops(text, is_gap_filled=is_gap_filled, debug=debug)
    
    # 2. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–µ–π
    has_dupl, text = is_duplicate_phrase(text, debug=debug)
    
    # 3. –£–¥–∞–ª–µ–Ω–∏–µ ending hallucinations
    text = remove_ending_hallucinations(text, debug=debug)
    
    # 4. –û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    text = re.sub(r'\s+', ' ', text)  # Multiple spaces ‚Üí one
    text = re.sub(r'([.!?]){2,}', r'\1', text)  # Multiple punctuation ‚Üí one
    text = text.strip()
    
    if debug and text != original_text:
        print(f"  ‚úÖ –û—á–∏—â–µ–Ω–æ: {len(original_text)} ‚Üí {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    return text


def filter_hallucination_segments(segments, debug=True):
    """
    üÜï v16.23: –§–∏–ª—å—Ç—Ä—É–µ—Ç —Å–µ–≥–º–µ–Ω—Ç—ã –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π (–æ–±–Ω–æ–≤–ª–µ–Ω–æ –¥–ª—è gap-filled)
    
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç clean_hallucinations_from_text() –∫ –∫–∞–∂–¥–æ–º—É —Å–µ–≥–º–µ–Ω—Ç—É.
    –£–¥–∞–ª—è–µ—Ç —Å–µ–≥–º–µ–Ω—Ç—ã, —Å—Ç–∞–≤—à–∏–µ –ø—É—Å—Ç—ã–º–∏ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏.
    
    Args:
        segments: –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        debug: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å debug output
    
    Returns:
        –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
    """
    if debug:
        print(f"\nüßπ –û—á–∏—Å—Ç–∫–∞ hallucinations –∏–∑ {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤...")
    
    cleaned_segments = []
    removed_count = 0
    
    for seg in segments:
        text = seg.get('text', '')
        speaker = seg.get('speaker', '')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–µ–≥–º–µ–Ω—Ç gap-filled
        is_gap_filled = seg.get('raw_speaker_id') == 'GAP_FILLED'
        
        cleaned_text = clean_hallucinations_from_text(
            text, 
            speaker, 
            is_gap_filled=is_gap_filled,
            debug=debug
        )
        
        if cleaned_text:
            seg['text'] = cleaned_text
            cleaned_segments.append(seg)
        else:
            removed_count += 1
            if debug:
                print(f"  üóëÔ∏è –£–¥–∞–ª—ë–Ω –ø—É—Å—Ç–æ–π —Å–µ–≥–º–µ–Ω—Ç: {seg.get('time', '???')} ({speaker})")
    
    if debug:
        print(f"‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(cleaned_segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (—É–¥–∞–ª–µ–Ω–æ {removed_count})")
    
    return cleaned_segments
